{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3416900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "\n",
    "import PIL\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import warnings \n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "from timm import create_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Inicializa modelos e dispositivo para uso\n",
    "\n",
    "#model_name = \"vit_base_patch16_224\"\n",
    "model_name = \"vit_base_patch32_224\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device = \", device)\n",
    "# create a ViT model : https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "model = create_model(model_name, pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base(name_arq):\n",
    "    \n",
    "    f = open(name_arq, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # Pega apenas as \"respondiveis\"\n",
    "    #data = [d for d in data if d[\"answerable\"] == 1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e371c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transform():\n",
    "    \n",
    "    # Define transforms for test\n",
    "    IMG_SIZE = (224, 224)\n",
    "    NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
    "    NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
    "    transforms = [\n",
    "                  T.Resize(IMG_SIZE),\n",
    "                  T.ToTensor(),\n",
    "                  T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "                  ]\n",
    "\n",
    "    transforms = T.Compose(transforms)\n",
    "    \n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vision_transformer(model, img_tensor):\n",
    "    \n",
    "    # Divide a imagem em patches\n",
    "    patches = model.patch_embed(img_tensor)\n",
    "    \n",
    "    # Calcula o vetor de positional embedding, para saber a posição correta de cada patch\n",
    "    pos_embed = model.pos_embed\n",
    "    \n",
    "    # Computa o embedding inicial juntamente com suas posições, além de trazer o embedding do token especial \"CLS\"\n",
    "    # que representa a imagem inteira e é o primeiro elemento do vetor\n",
    "    transformer_input = torch.cat((model.cls_token, patches), dim=1) + pos_embed\n",
    "    \n",
    "    # Calcula o embedding final, passando pelos blocos de encoder do modelo\n",
    "    x = transformer_input.clone()\n",
    "    for i, blk in enumerate(model.blocks):\n",
    "        x = blk(x)\n",
    "    x = model.norm(x)\n",
    "    \n",
    "    # x é o embedding que irá representar os patches e imagem, sendo na seguinte ordem: imagem, patch 0, patch 1, etc.\n",
    "    #x = x.reshape(197,768)\n",
    "    x = x.reshape(50,768)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visão\n",
    "\"\"\"\n",
    "{\"imagem.jpg\": [vetores], \n",
    "\"imagem2.jpg\": [vetores],\n",
    "...\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d57589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_visao(name_arq, model, tam_base):\n",
    "    \n",
    "    # Realiza a leitura da base\n",
    "    data = load_base(name_arq)\n",
    "    \n",
    "    #data = data[:tam_base]\n",
    "    \n",
    "    # Define diretório onde se encontram as imagens\n",
    "    dir_img = name_arq.split(\".json\")[0]\n",
    "    \n",
    "    # Define padronização que será feita nas imagens\n",
    "    transforms = define_transform()\n",
    "    \n",
    "    # Irá carregar as informações visuais referentes a cada uma das imagens\n",
    "    info_visao = {}\n",
    "    \n",
    "    for info in tqdm(data):\n",
    "        \n",
    "        # Pega o nome da imagem\n",
    "        name_img = info[\"image\"]\n",
    "        \n",
    "        # Faz a leitura da imagem\n",
    "        img = PIL.Image.open(dir_img+\"/\"+name_img)\n",
    "        \n",
    "        # Padroniza a imagem\n",
    "        img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Pega os embeddings referentes a imagem\n",
    "        embedding_img = get_embedding_vision_transformer(model, img_tensor)\n",
    "        \n",
    "        # Atualiza as informações da imagem\n",
    "        info_visao[name_img] = embedding_img.detach().numpy()\n",
    "        \n",
    "    return info_visao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_visao(name_arq_in, info_visao):\n",
    "    \n",
    "    name_arq_out = name_arq_in.split(\".json\")[0]+\"_info_visao\"\n",
    "    \n",
    "    file = open(name_arq_out, 'wb')\n",
    "    pickle.dump(info_visao, file)                   \n",
    "    file.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name_arq = \"test.json\"\n",
    "info_visao = get_info_visao(name_arq, model, 160)\n",
    "save_info_visao(name_arq, info_visao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b5c03",
   "metadata": {},
   "source": [
    "#### Fontes\n",
    "\n",
    "https://colab.research.google.com/drive/1jIflL9-gktbXq_2cEE_KM7yYq2PaOKRM?authuser=1#scrollTo=8cRyNhMQaAyh\n",
    "\n",
    "https://khvmaths.medium.com/vision-transformer-understanding-the-underlying-concept-83d699d71180"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
