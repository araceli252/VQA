{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Visual Question Answering](#toc1_)    \n",
    "  - [Installing Required Libraries](#toc1_1_)    \n",
    "  - [Importing Libraries](#toc1_2_)    \n",
    "  - [Configuring the Notebook](#toc1_3_)    \n",
    "  - [Processing Data](#toc1_4_)    \n",
    "  - [Creating Dataframes & Splitting](#toc1_5_)    \n",
    "  - [Exploratory Data Analysis](#toc1_6_)    \n",
    "    - [Training Dataframe](#toc1_6_1_)    \n",
    "    - [Validation Dataframe](#toc1_6_2_)    \n",
    "    - [Testing Dataframe](#toc1_6_3_)    \n",
    "  - [Processing Images & Questions using CLIP model](#toc1_7_)    \n",
    "  - [Creating Dataset Class](#toc1_8_)    \n",
    "  - [Building Model's Architecture](#toc1_9_)    \n",
    "  - [Loading Preprocessed Embeddings](#toc1_10_)    \n",
    "  - [Preparing Data Loaders](#toc1_11_)    \n",
    "  - [Training the Model](#toc1_12_)    \n",
    "  - [Remarks](#toc1_13_)    \n",
    "  - [Test your own image !](#toc1_14_)    \n",
    "  - [Building Test Answers](#toc1_15_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to make sure to install the required libraries. We will use [PyTorch](https://pytorch.org/) for building our model. We will also [Open AI's CLIP](https://openai.com/research/clip) pretrained model for image and text embedding which is open sourced on [GitHub](https://github.com/openai/CLIP). We will use [LaTeX](https://www.latex-project.org/) for writing our research [report](https://github.com/yousefkotp/Visual-Question-Answering/blob/9c27560e9c19a0981343fd5fce25861236ab939f/LaTeX_Paper/Visual_Question_Answering_Report.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2023-06-22T22:07:22.997395Z",
     "shell.execute_reply": "2023-06-22T22:07:22.995390Z",
     "shell.execute_reply.started": "2023-06-22T22:07:05.284610Z"
    }
   },
   "source": [
    "%pip install ftfy regex tqdm --user\n",
    "%pip install pandas --user\n",
    "%pip install wordcloud --user\n",
    "%pip install sklearn --user\n",
    "%pip install scikit-learn --user\n",
    "%pip install Levenshtein --user\n",
    "%pip install git+https://github.com/openai/CLIP.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importing Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required libraries\n",
    "#!pip uninstall -r requirements.txt -y \n",
    "#!pip install -r requirements2.txt\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install requests\n",
    "!pip install regex\n",
    "!pip install pillow\n",
    "!pip install pandas\n",
    "!pip install opencv-python-headless\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install zipp\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install torch_geometric\n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install spacy\n",
    "!pip install spacy-alignments\n",
    "!pip install spacy-curated-transformers\n",
    "!pip install spacy-legacy\n",
    "!pip install spacy-loggers\n",
    "!pip install wget\n",
    "!pip install spacy-transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-Levenshtein\n",
    "!pip install protobuf\n",
    "!pip3 install torchaudio\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#git clone https://github.com/salesforce/LAVIS.git\n",
    "#cd LAVIS\n",
    "#pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "import wget\n",
    "\n",
    "zip_file_test = \"https://vizwiz.cs.colorado.edu/VizWiz_final/images/test.zip\"\n",
    "filename_test = wget.download(zip_file_test)\n",
    "\n",
    "zip_file_val =\"https://vizwiz.cs.colorado.edu/VizWiz_final/images/val.zip\"\n",
    "filename_val = wget.download(zip_file_val)\n",
    "\n",
    "zip_file_train =\"https://vizwiz.cs.colorado.edu/VizWiz_final/images/train.zip\"\n",
    "filename_train = wget.download(zip_file_train)\n",
    "\n",
    "# Extract dataset\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"test.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"test\")\n",
    "\n",
    "with zipfile.ZipFile(\"val.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"val\")\n",
    "\n",
    "with zipfile.ZipFile(\"train.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing os, numpy and pandas for data manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data visualization, we will use matplotlib, wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data preprocessing, we will use Counter, train_test_split, Levenshtein distance, Python Image Library and OneHotEncoder\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving and loading the preprocessed data, we will use pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For taking the image from the URL, we will use requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we will need sklearn.metrics.average_precision_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing json for results formatting which will be uploaded for evaluation\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Building the model, we will use PyTorch and its functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import clip\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain_vitL\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the paths for the dataset\n",
    "#INPUT_PATH = '/kaggle/input/vizwiz'\n",
    "#ANNOTATIONS = INPUT_PATH + '/Annotations/Annotations'\n",
    "TRAIN_PATH = 'train/train'\n",
    "VALIDATION_PATH = 'val/val'\n",
    "ANNOTATIONS_TRAIN_PATH = 'train.json'\n",
    "ANNOTATIONS_VAL_PATH = 'val.json'\n",
    "OUTPUT_PATH = 'saida_blip2/'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "# Using accelerated computing if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines functions for exploratory data analysis and preprocessing of data\n",
    "- `read_dataframe` function reads a JSON file and returns a dataframe with required columns\n",
    "- `split_train_test` function splits the dataframe into train and test sets\n",
    "- `plot_histogram`, `plot_pie`, and `plot_wordcloud` functions plot the histogram, pie chart, and wordcloud of the given column, respectively\n",
    "- `explore_dataframe` function explores the dataframe by utilizing the previous functions\n",
    "- `get_number_of_distinct_answers` function returns the number of distinct answers in the dataframe\n",
    "- `process_images` function processes the images in the dataframe and returns the image features using Open AI's CLIP model\n",
    "- `process_questions` function processes the questions in the dataframe and returns the question features using Open AI's CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and returns a dataframe with the required columns (image, question, answers, answer_type, answerable)\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Dataframe with the required columns\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "    df = df[['image', 'question', 'answers', 'answer_type', 'answerable']]\n",
    "    return df\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be split\n",
    "\n",
    "    Returns:\n",
    "        train (pandas.DataFrame): Train set\n",
    "        test (pandas.DataFrame): Test set\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "def get_number_of_distinct_answers(dataframe):\n",
    "    \"\"\"\n",
    "    Returns the number of distinct answers in the dataframe\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        len(unique_answers_set) (int): Number of distinct answers in the dataframe\n",
    "    \"\"\"\n",
    "    unique_answers_set = set()\n",
    "    for row in dataframe['answers']:\n",
    "        for answer_map in row:\n",
    "            unique_answers_set.add(answer_map['answer'])\n",
    "    return len(unique_answers_set)\n",
    "\n",
    "def process_images(dataframe, image_path, model, vis_processors, device):\n",
    "    \"\"\"\n",
    "    Processes the images in the dataframe and returns the image features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the images\n",
    "        image_path (str): Path to the input images\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        preprocessor (clip.model.Preprocess): Preprocessor for the CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        images (list): List of image features\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        full_path = image_path + \"/\" + row['image']\n",
    "        image = Image.open(full_path)\n",
    "        #preprocess image\n",
    "        image_processed = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n",
    "        sample = {\"image\": image_processed}\n",
    "        image_features = model.extract_features(sample, mode=\"image\").image_embeds[0,0,:] # size (768)\n",
    "        #print(image_features.shape) \n",
    "    #  image_features = torch.flatten(image_features, start_dim=1)\n",
    "        images.append(image_features)\n",
    "    return images\n",
    "\n",
    "def process_questions(dataframe, model, txt_processors):\n",
    "    \"\"\"\n",
    "    Processes the questions in the dataframe and returns the question features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the questions\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        questions (list): List of question features\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['question']\n",
    "        text_input = txt_processors[\"eval\"](question)\n",
    "        # build sample\n",
    "        sample = {\"text_input\": [text_input]}\n",
    "        text_features = model.extract_features(sample, mode=\"text\").text_embeds[0,0,:] # size (768)\n",
    "        #text_features = torch.flatten(text_features, start_dim=1)\n",
    "        questions.append(text_features)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creating Dataframes & Splitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use previously defined functions to create dataframes and split them into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "train_df, test_df = split_train_test(train_df, test_size=0.05)\n",
    "ANSWER_SPACE = get_number_of_distinct_answers(train_df) # The answer space will be decreased later when we process the answers\n",
    "print(\"Number of distinct answers: \", ANSWER_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Processing Images & Questions using CLIP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of lazy processing of images and questions embeddings and recomputing them over and over during forward passes in the model, we can preprocess them and save them in a file using Pickle. This will save us a lot of time when we want to train our model and decrease the time taken by one epoch drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_model, preprocessor = clip.load(MODEL_NAME, device = DEVICE)\n",
    "#clip_model.eval().requires_grad_(False)\n",
    "\n",
    "training_images = process_images(train_df, TRAIN_PATH, model, vis_processors, DEVICE)\n",
    "training_questions = process_questions(train_df, model, txt_processors)\n",
    "with open(OUTPUT_PATH + 'training_images.pkl', 'wb') as f:\n",
    "    pickle.dump(training_images, f)\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(training_questions, f)\n",
    "\n",
    "validation_images = process_images(validation_df, VALIDATION_PATH, model, vis_processors, DEVICE)\n",
    "validation_questions = process_questions(validation_df, model, txt_processors)\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_images, f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_questions, f)\n",
    "\n",
    "test_images = process_images(test_df, TRAIN_PATH, model, vis_processors, DEVICE)\n",
    "test_questions = process_questions(test_df, model, txt_processors)\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'wb') as f:\n",
    "    pickle.dump(test_images, f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(test_questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Creating Dataset Class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch requires using Dataset class. We will create a class that will be used to load the data and process it during training. We will also use this class to load the preprocessed images and questions embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, dataframe, answer_type_onehotencoder = None, answer_onehotencoder = None, model_name = \"RN50x64\", images_features = torch.tensor([]), questions_features = torch.tensor([])):\n",
    "        super(VizWizDataset, self).__init__()\n",
    "\n",
    "        # Total counter for all answers before filtering, used in Tie Breaking when building the answer vocabulary\n",
    "        self.answer_counter = Counter()\n",
    "\n",
    "        # Saving image & question embeddings\n",
    "        self.images_features = images_features\n",
    "        self.questions_features = questions_features\n",
    "        self.answerable = dataframe['answerable'].to_numpy()\n",
    "\n",
    "        # Saving the dataframe\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        # List for answers for each question (each question has 10 answers)\n",
    "        self.answer_counter_per_question = []\n",
    "\n",
    "        # Populating the counter for words in answers which will be used when building answer vocabulary\n",
    "        self.build_answer_counter()\n",
    "\n",
    "        # Building the answer vocabulary according to the methodology explained in the paper\n",
    "        self.build_answer_vocab()\n",
    "\n",
    "        # The number of vocabulary words after filtering\n",
    "        print(\"Number of distinct answers: \", len(self.get_answer_vocab()))\n",
    "\n",
    "        # One hot encoding the answers\n",
    "        if answer_type_onehotencoder is None:\n",
    "            answer_type_onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            answer_type_onehotencoder.fit(self.copied_dataframe[['answer_type']])\n",
    "\n",
    "        # One hot encoding the answer types\n",
    "        if answer_onehotencoder is None:\n",
    "            answer_onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            answer_onehotencoder.fit(self.copied_dataframe[['answer']])\n",
    "\n",
    "        # Saving the one hot encoders\n",
    "        self.answer_onehotencoder = answer_onehotencoder\n",
    "        self.answer_type_onehotencoder = answer_type_onehotencoder\n",
    "\n",
    "        # Transforming the answers and answer types to one hot encoded vectors\n",
    "        self.answer_onehotencoded = answer_onehotencoder.transform(self.copied_dataframe[['answer']]).toarray()\n",
    "        self.answer_type_onehotencoded = answer_type_onehotencoder.transform(self.copied_dataframe[['answer_type']]).toarray()\n",
    "\n",
    "        # Saving the answer categories (vocabulary) which will be used when getting index of the predicted answer\n",
    "        self.answers_categories = self.answer_onehotencoder.categories_[0].tolist()\n",
    "\n",
    "        # Saving answers for each question (each question has 10 answers)\n",
    "        self.build_answer_counter_per_question()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        answer = torch.tensor(self.answer_onehotencoded[index], dtype=torch.float32)\n",
    "        answer_type = torch.tensor(self.answer_type_onehotencoded[index], dtype=torch.float32)\n",
    "        answer_counter = torch.tensor(self.answer_counter_per_question[index], dtype=torch.long)\n",
    "        answerable = torch.tensor(self.answerable[index], dtype=torch.float32)\n",
    "        return self.images_features[index], self.questions_features[index], answer, answer_type, answer_counter, answerable\n",
    "\n",
    "    def build_answer_counter_per_question(self):\n",
    "\n",
    "        for index, row in self.dataframe.iterrows():\n",
    "            temp_list = []\n",
    "            for answer_map in row['answers']:\n",
    "                answer = answer_map['answer']\n",
    "                # check if answer in self.answers_categories\n",
    "                if answer in self.answers_categories:\n",
    "                    answer_index = self.answers_categories.index(answer)\n",
    "                    temp_list.append(answer_index)\n",
    "            # Torch.tensor requires the all the lists to have constant length, so we pad the list with -1 if needed\n",
    "            while len(temp_list) < 10:\n",
    "                temp_list.append(-1)\n",
    "            self.answer_counter_per_question.append(temp_list)\n",
    "\n",
    "\n",
    "\n",
    "    def build_answer_vocab(self):\n",
    "        # Building answer vocab follow this policy:\n",
    "        # for each question we have 10 answers, we choose the most frequent answer as the answer for this question\n",
    "        # if there is a tie, we choose the most common one in the whole dataset\n",
    "        # if there is a tie, we choose the pairwise Levenshtein distance is used to find the answer that is most representative to all others.\n",
    "\n",
    "        # Copying the original dataframe which will be manipulated\n",
    "        self.copied_dataframe = self.dataframe.copy()\n",
    "        self.copied_dataframe.drop(columns=['answers'], inplace=True)\n",
    "\n",
    "        # Adding extra column named 'answer'\n",
    "        self.copied_dataframe['answer'] = None\n",
    "\n",
    "        for index, row in self.dataframe.iterrows():\n",
    "            intermediate_counter = Counter()\n",
    "            for answer_map in row['answers']:\n",
    "                answer = answer_map['answer']\n",
    "                intermediate_counter.update([answer])\n",
    "\n",
    "            # let's see the top elements in the answers_counter to check if there is a tie\n",
    "            top_elements = intermediate_counter.most_common(1)\n",
    "            if len(top_elements) == 1:\n",
    "                self.copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "            else:\n",
    "                # let's see who is the most common answer in the whole dataset\n",
    "                top_elements = self.answer_counter.most_common(1)\n",
    "                if len(top_elements) == 1:\n",
    "                    self.copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "                else:\n",
    "                    # let's get the minimum levenshtein distance between the answers in top_elements\n",
    "                    current_min = np.inf\n",
    "                    current_answer = None\n",
    "                    for answer in top_elements:\n",
    "                        total_distance = 0\n",
    "                        for answer2 in top_elements:\n",
    "                            if answer != answer2:\n",
    "                                lev_distance = lev.distance(answer[0], answer2[0])\n",
    "                                total_distance += lev_distance\n",
    "                        if total_distance < current_min:\n",
    "                            current_min = total_distance\n",
    "                            current_answer = answer[0]\n",
    "                    self.copied_dataframe.at[index, 'answer'] = current_answer\n",
    "        return\n",
    "\n",
    "    def build_answer_counter(self):\n",
    "        for row in self.dataframe['answers']:\n",
    "            for answer_map in row:\n",
    "                self.answer_counter.update([answer_map['answer']])\n",
    "\n",
    "    def get_answer_vocab(self):\n",
    "        return self.copied_dataframe['answer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Building Model's Architecture](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our model's architecture according to the paper. We will use PyTorch to build our model as we said before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, hidden_size, model, vis_processors, txt_processors, device = torch.device(\"cpu\")):\n",
    "        super(VQAModel, self).__init__()\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "\n",
    "        self.vizwiz_training_accuracies = []\n",
    "        self.vizwiz_validation_accuracies = []\n",
    "\n",
    "        self.training_answerability = []\n",
    "        self.validation_answerability = []\n",
    "\n",
    "        self.device = device\n",
    "        #self.model_name = model_name\n",
    "\n",
    "        # Initializing Binary Cross Entropy Loss which will be used to train the model on answerability\n",
    "        self.answerability_loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Loading the CLIP model\n",
    "        #self.clip_model, self.preprocess = clip.load(model_name, device = device)\n",
    "\n",
    "        # Freezing the CLIP model\n",
    "        #for param in self.clip_model.parameters():\n",
    "        #    param.requires_grad = False\n",
    "\n",
    "        # First linear layer\n",
    "        self.linear_layer1 = nn.Sequential(\n",
    "            nn.LayerNorm(1536),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1536, hidden_size)\n",
    "        )\n",
    "\n",
    "        # Second linear layer\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "        self.answer_type_layer = nn.Linear(hidden_size, 4)\n",
    "        self.answer_mask_layer = nn.Linear(4, num_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Answerability Linear Layer (We removed drop out layer because training answerability was very bad)\n",
    "        self.answerability_linear_layer = nn.Sequential(\n",
    "            nn.LayerNorm(1536),\n",
    "            nn.Linear(1536, hidden_size)\n",
    "        )\n",
    "\n",
    "        # Answerability Sigmoid Layer\n",
    "        self.answerability_final_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Sigmoid Layer for Answerability\n",
    "        self.answerability_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image, question):\n",
    "\n",
    "        # Flattening and concatenating the image and question features\n",
    "        image = torch.flatten(image, start_dim=1)\n",
    "        question = torch.flatten(question, start_dim=1)\n",
    "        features = torch.cat((image, question), dim=1)\n",
    "\n",
    "        # Calculating the answerability score\n",
    "        answerability_score = self.answerability_linear_layer(features)\n",
    "        answerability_score = self.answerability_final_layer(answerability_score)\n",
    "        answerability_score = self.answerability_sigmoid(answerability_score)\n",
    "        answerability_score = answerability_score.squeeze()\n",
    "\n",
    "        # Passing the features through the first linear layer\n",
    "        features = self.linear_layer1(features)\n",
    "\n",
    "        # Passing the features to get 4 answer types\n",
    "        answer_type = self.answer_type_layer(features)\n",
    "\n",
    "        # Expanding answer make to the same size as the number of classes (vocab size)\n",
    "        answer_mask = self.answer_mask_layer(answer_type)\n",
    "\n",
    "        # Applying sigmoid to get the answer mask\n",
    "        answer_mask = self.sigmoid(answer_mask)\n",
    "\n",
    "        # Passing the features through the second linear layer\n",
    "        output = self.linear_layer2(features)\n",
    "\n",
    "        # Applying the answer mask to the output\n",
    "        output = output * answer_mask\n",
    "\n",
    "        return output, answer_type, answerability_score\n",
    "\n",
    "    def train_model(self, training_dataloader, validation_dataloader, test_dataloader, criterion, optimizer, epochs = 10, save_path = None, save_every = 1):\n",
    "        for epoch in range(1,epochs+1):\n",
    "            training_loss, training_accuracy, training_vizwiz_accuracy, train_answerability_score = self.training_step(training_dataloader, criterion, optimizer, self.device)\n",
    "            validation_loss, validation_accuracy, validation_vizwiz_accuracy, validation_answerability_score = self.validation_step(validation_dataloader, criterion, self.device)\n",
    "            test_accuracy, test_vizwiz_accuracy, test_answerability_score = self.test_step(test_dataloader)\n",
    "\n",
    "            self.training_losses.append(training_loss)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "            self.training_accuracies.append(training_accuracy)\n",
    "            self.validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "            self.vizwiz_training_accuracies.append(training_vizwiz_accuracy)\n",
    "            self.vizwiz_validation_accuracies.append(validation_vizwiz_accuracy)\n",
    "\n",
    "            self.training_answerability.append(train_answerability_score)\n",
    "            self.validation_answerability.append(validation_answerability_score)\n",
    "\n",
    "\n",
    "            print(\"Epoch: {} | Training Loss: {:.3f} | Validation Loss: {:.3f}\".format(epoch, training_loss, validation_loss))\n",
    "            print(\"Epoch: {} | Training Accuracy: {:.3f} | Validation Accuracy: {:.3f} | Test Accuracy: {:.3f}\".format(epoch, training_accuracy, validation_accuracy, test_accuracy))\n",
    "            print(\"Epoch: {} | Training VizWiz Accuracy: {:.3f} | Validation VizWiz Accuracy: {:.3f} | Test VizWiz Accuracy: {:.3f}\".format(epoch, training_vizwiz_accuracy, validation_vizwiz_accuracy, test_vizwiz_accuracy))\n",
    "            print(\"Epoch: {} | Training Answerability Score: {:.3f} | Validation Answerability Score: {:.3f} | Test Answerability Score: {:.3f}\\n\".format(epoch, train_answerability_score, validation_answerability_score, test_answerability_score))\n",
    "\n",
    "            if save_path != None and epoch % save_every == 0:\n",
    "                self.save_model(save_path + \"epoch_{}.pth\".format(epoch))\n",
    "        return\n",
    "\n",
    "    def training_step(self, dataloader, criterion, optimizer, device):\n",
    "        training_loss, training_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        self.train()\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "            image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "            answerable = 1 - answerable\n",
    "            answerable_predict = 1.0 - answerable_predict            \n",
    "            loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "            predicted_answer = torch.argmax(output, dim = 1)\n",
    "            actual_answer = torch.argmax(answer, dim = 1)\n",
    "            for i in range(len(answer)):\n",
    "                if actual_answer[i] == predicted_answer[i]:\n",
    "                    training_accuracy +=1\n",
    "                total_sum +=1\n",
    "                vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                answerable_true.append(answerable[i].item())\n",
    "                answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        training_loss /= len(dataloader)\n",
    "        training_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        return training_loss, training_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "\n",
    "    def validation_step(self, dataloader, criterion, device):\n",
    "        validation_loss, validation_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "                output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "\n",
    "                # Answerablity is the confidence that quesion is not answerable, so we have to subtract from 1\n",
    "                answerable = 1 - answerable\n",
    "                answerable_predict = 1.0 - answerable_predict\n",
    "                loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "                validation_loss += loss.item()\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                    if torch.sum(answer[i]) == 0:\n",
    "                        continue\n",
    "                    if actual_answer[i] == predicted_answer[i]:\n",
    "                        validation_accuracy += 1\n",
    "                    total_sum +=1\n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    answerable_true.append(answerable[i].item())\n",
    "                    answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        validation_loss /= len(dataloader)\n",
    "        validation_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        # We will use weighted average since that there is imbalance in answerability in the dataset as displayed in EDA section\n",
    "        return validation_loss, validation_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "    def test_step(self, dataloader):\n",
    "        self.eval()\n",
    "        accuracy, total_sum, vizwiz_accuracy = 0.0, 0, 0.0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = image.to(self.device), question.to(self.device), answer.to(self.device), answer_type.to(self.device), answers_for_questions.to(self.device), answerable.to(self.device)\n",
    "                output, _, answerable_predict = self.forward(image, question)\n",
    "                answerable = 1 - answerable\n",
    "                answerable_predict = 1.0 - answerable_predict\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                    if torch.sum(answer[i]) == 0:\n",
    "                        continue\n",
    "                    if predicted_answer[i] == actual_answer[i]:\n",
    "                        accuracy += 1\n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    total_sum +=1\n",
    "                    answerable_true.append(answerable[i].item())\n",
    "                    answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "        return accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model state dictionary to the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to save the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model state dictionary from the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to load the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - self: the loaded model object\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        return self\n",
    "\n",
    "    def predict(self, image, question):\n",
    "        \"\"\"\n",
    "        Predicts the output and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image (tensor): the image tensor\n",
    "        - question (tensor): the question tensor\n",
    "\n",
    "        Returns:\n",
    "        - output (tensor): the predicted output tensor\n",
    "        - answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        output, answer_type, answerability = self.forward(image, question)\n",
    "        answerability = 1.0 - answerability\n",
    "        return output, answer_type, answerability\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation losses.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_losses, label = \"Training Loss\")\n",
    "        plt.plot(self.validation_losses, label = \"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation accuracies.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_accuracies, label = \"Training Accuracy\")\n",
    "        plt.plot(self.validation_accuracies, label = \"Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_vizwiz_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plots the VizWiz training and validation accuracies.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.vizwiz_training_accuracies, label = \"VizWiz Training Accuracy\")\n",
    "        plt.plot(self.vizwiz_validation_accuracies, label = \"VizWiz Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_answerability(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation answerabilities.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_answerability, label = \"Training Answerability\")\n",
    "        plt.plot(self.validation_answerability, label = \"Validation Answerability\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def test_model(self, image_path, question, model, vis_processors, txt_processors):\n",
    "        \"\"\"\n",
    "        Tests the model by predicting the answer and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image_path (str): the path to the image file or URL\n",
    "        - question (str): the question to be asked\n",
    "\n",
    "        Returns:\n",
    "        - predicted_answer (tensor): the predicted answer tensor\n",
    "        - predicted_answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        if image_path.startswith(\"http\"):\n",
    "            image = Image.open(requests.get(image_path, stream = True).raw)\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "        image = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n",
    "        sample = {\"image\": image}\n",
    "        image_features = model.extract_features(sample, mode=\"image\").image_embeds[0,0,:] # size (768)\n",
    "        #image_features = torch.flatten(image_features, start_dim=1)\n",
    "        image_features = image_features.reshape(1,-1)\n",
    "\n",
    "        question =  txt_processors[\"eval\"](question)\n",
    "        # build sample\n",
    "        sample = {\"text_input\": [question]}\n",
    "        text_features = model.extract_features(sample, mode=\"text\").text_embeds[0,0,:] # size (768)\n",
    "        #text_features = torch.flatten(text_features, start_dim=1)\n",
    "        text_features = text_features.reshape(1,-1)\n",
    "\n",
    "        predicted_answer, predicted_answer_type, answerability = self.predict(image_features, text_features)\n",
    "        return predicted_answer, predicted_answer_type, answerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Loading Preprocessed Embeddings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_PATH + 'training_images.pkl', 'rb') as f:\n",
    "    training_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'rb') as f:\n",
    "    training_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'rb') as f:\n",
    "    validation_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'rb') as f:\n",
    "    validation_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'rb') as f:\n",
    "    test_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'rb') as f:\n",
    "    test_questions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[Preparing Data Loaders](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the training dataset\n",
    "training_dataset = VizWizDataset(train_df, None, None, MODEL_NAME, training_images, training_questions)\n",
    "ANSWER_ONEHOTENCODER = training_dataset.answer_onehotencoder\n",
    "ANSWER_TYPE_ONEHOTENCODER = training_dataset.answer_type_onehotencoder\n",
    "\n",
    "# Saving the fitted one hot encoders\n",
    "with open(OUTPUT_PATH + 'answer_onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(ANSWER_ONEHOTENCODER, f)\n",
    "with open(OUTPUT_PATH + 'answer_type_onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(ANSWER_TYPE_ONEHOTENCODER, f)\n",
    "\n",
    "# Constructing the validation dataset\n",
    "validation_dataset = VizWizDataset(validation_df, ANSWER_TYPE_ONEHOTENCODER, ANSWER_ONEHOTENCODER, MODEL_NAME, validation_images, validation_questions)\n",
    "\n",
    "# Constructing the test dataset\n",
    "test_dataset = VizWizDataset(test_df, ANSWER_TYPE_ONEHOTENCODER, ANSWER_ONEHOTENCODER, MODEL_NAME, test_images, test_questions)\n",
    "\n",
    "# Configuring the data loaders\n",
    "BATCH_SIZE = 32 # 64 is good too but 32 is better (variance wise)\n",
    "\n",
    "# Constructing the training, validation and test data loaders\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[Training the Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training's hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_CLASSES = len(training_dataset.get_answer_vocab())\n",
    "SAVE_PATH = OUTPUT_PATH\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "# Initializing the model\n",
    "model_vqa = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=512, model = model, vis_processors = vis_processors, txt_processors = txt_processors).to(DEVICE)\n",
    "#model.print_CLIP_model()\n",
    "\n",
    "# Initializing the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model_vqa.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "# Training the model and plotting the loss and accuracy\n",
    "model_vqa.train_model(training_dataloader, validation_dataloader, test_dataloader, loss_function, optimizer, epochs=NUM_EPOCHS, save_path=SAVE_PATH, save_every=SAVE_EVERY)\n",
    "model_vqa.plot_loss()\n",
    "model_vqa.plot_accuracy()\n",
    "model_vqa.plot_vizwiz_accuracy()\n",
    "model_vqa.plot_answerability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model at **epoch number 45** outperforms the same model at any other epochs, so let's pick this model as our ultimate and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_13_'></a>[Remarks](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, the model is very light weight and fast, it takes ~ 1 minute to run an epoch. In addition to this, the model converges very fast, it only takes a maximum of 30 epoch to fully converges. This is due to the fact that we are using CLIP model which is pretrained on a huge dataset.\n",
    "- We can further improve the model by training more models with same architecture but different backbone for CLIP model. We can also use different pretrained models for image and text embeddings and ensemble them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_14_'></a>[Test your own image !](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part of code allows the user to test his own image using the trained model. You just have to configure `IMAGE_PATH` and `QUESTION` variables and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a sample image and question from the user\n",
    "QUESTION = \"How many calories?\"\n",
    "IMAGE_PATH = \"train/train/VizWiz_train_00014962.jpg\"\n",
    "\n",
    "# Loading the fitted One Hot Encoders from the disk\n",
    "with open(OUTPUT_PATH + 'answer_onehotencoder.pkl', 'rb') as f:\n",
    "    ANSWER_ONEHOTENCODER = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'answer_type_onehotencoder.pkl', 'rb') as f:\n",
    "    ANSWER_TYPE_ONEHOTENCODER = pickle.load(f)\n",
    "\n",
    "# Loading the model from the disk\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"ViT-L/14@336px\"\n",
    "NUM_CLASSES = len(training_dataset.get_answer_vocab())\n",
    "MODEL_PATH = \"saida_blip2/epoch_50.pth\" # OUTPUT_PATH + 'model.pt'\n",
    "model_vqa = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=512, model = model, vis_processors = vis_processors, txt_processors = txt_processors).to(DEVICE)\n",
    "model_vqa.load_model(MODEL_PATH)\n",
    "\n",
    "# Predicting the answer and answer type\n",
    "predicted_answer, predicted_answer_type, answerability = model_vqa.test_model(image_path = IMAGE_PATH, question = QUESTION, model = model, vis_processors = vis_processors, txt_processors = txt_processors)\n",
    "answer = ANSWER_ONEHOTENCODER.inverse_transform(predicted_answer.cpu().detach().numpy())\n",
    "answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "\n",
    "# Printing the answer and answer type\n",
    "print(\"The Answer is: \" + answer[0][0])\n",
    "print(\"The Answer Type is: \" + answer_type[0][0])\n",
    "print(\"The confidence for being unanswerable: \" + str(answerability.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_15_'></a>[Building Test Answers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"test.json\")\n",
    "df = df[['image', 'question']]\n",
    "\n",
    "# let's create two json objects to store the output of the model then write them to a file\n",
    "model_answers = []\n",
    "model_answerability = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    image_url = df['image'][i]\n",
    "    question = df['question'][i]\n",
    "    image_path = \"test/test/\" + image_url\n",
    "    predicted_answer, predicted_answer_type, answerability = model_vqa.test_model(image_path = image_path, question = question, model = model, vis_processors = vis_processors, txt_processors = txt_processors)\n",
    "    answer = ANSWER_ONEHOTENCODER.inverse_transform(predicted_answer.cpu().detach().numpy())\n",
    "    answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "    answer_result = {'image': image_url, 'answer': answer[0][0]}\n",
    "    answerability_result = {'image': image_url, 'answerability': answerability.item()}\n",
    "    model_answers.append(answer_result)\n",
    "    model_answerability.append(answerability_result)\n",
    "\n",
    "# Writing them using pickle\n",
    "with open('answers_results.json', 'w') as file:\n",
    "    json.dump(model_answers, file)\n",
    "with open('answerability_results.json', 'w') as file:\n",
    "    json.dump(model_answerability, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
