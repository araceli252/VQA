{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "\n",
    "# Seed the behavior of the environment variable\n",
    "os.environ['PYTHONHASHSEED'] = str(1)\n",
    "# Seed numpy's instance in case you are using numpy's random number generator, shuffling operations, ...\n",
    "np.random.seed(1)\n",
    "\n",
    "# In general seed PyTorch operations\n",
    "torch.manual_seed(0)\n",
    "# If you are using CUDA on 1 GPU, seed it\n",
    "torch.cuda.manual_seed(0)\n",
    "# If you are using CUDA on more than 1 GPU, seed them all\n",
    "torch.cuda.manual_seed_all(0)\n",
    "# Disable the inbuilt cudnn auto-tuner that finds the best algorithm to use for your hardware.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Certain operations in Cudnn are not deterministic, and this line will force them to behave!\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980106e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(112,112)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a64fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.ImageFolder('train', transform = transform)\n",
    "data_val = datasets.ImageFolder('val', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size = 32, shuffle = True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(data_val, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dispositivo utilizado para treinamento (GPU/ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo sendo usado: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aabb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            #nn.Linear(2*16*c_hid, latent_dim)\n",
    "            nn.Linear(2*196*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            nn.Linear(latent_dim, 2*196*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        #x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = x.reshape(x.shape[0], -1, 14, 14)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int = 112,\n",
    "                 latent_dim: int = 768,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3,\n",
    "                 width: int = 112,\n",
    "                 height: int = 112):\n",
    "        super().__init__()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3080a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o pipeline de treinamento\n",
    "def pipeline_train(model, loader, device, optimizer, base):\n",
    "\n",
    "  base_loss = 0.0\n",
    "\n",
    "  for data in loader:\n",
    "\n",
    "        # Transfer Data to GPU if available\n",
    "        x = data[0].to(device)\n",
    "\n",
    "        if base == \"train\":\n",
    "          # Clear the gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        x_hat = model(x)\n",
    "\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "\n",
    "        if base == \"train\":\n",
    "\n",
    "          # Calculate gradients\n",
    "          loss.backward()\n",
    "          # Update Weights\n",
    "          optimizer.step()\n",
    "\n",
    "        # Calculate Loss\n",
    "        base_loss += loss.item()\n",
    "\n",
    "  return base_loss, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza o treinamento do modelo\n",
    "\n",
    "# Declaring Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                 mode='min',\n",
    "                                                 factor=0.2,\n",
    "                                                 patience=20,\n",
    "                                                 min_lr=5e-5)\n",
    "\"\"\"\n",
    "# Training with Validation\n",
    "epochs = 200\n",
    "\n",
    "start = time.time()\n",
    "for e in tqdm(range(epochs)):\n",
    "\n",
    "    model.train()\n",
    "    train_loss, model, optimizer = pipeline_train(model, train_loader, device, optimizer,\n",
    "                                                      \"train\")\n",
    "\n",
    "    loss_train_mean = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    val_loss, model, optimizer = pipeline_train(model, valid_loader, device, optimizer,\n",
    "                                                    \"val\")\n",
    "    #scheduler.step(val_loss)\n",
    "    loss_val_mean = val_loss / len(valid_loader)\n",
    "\n",
    "    print(f'Epoch {e} \\t Training Loss: {loss_train_mean} \\t Validation Loss: {loss_val_mean}')\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, \"autoencoder.ckpt\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d25ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images(num):\n",
    "    return torch.stack([data_train[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7872676",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_train_images(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81872c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4904ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_train[0][0].numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xhat[0, :, :, :]\n",
    "y = y.permute(1,2,0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Validation\n",
    "epochs = 200\n",
    "\n",
    "start = time.time()\n",
    "for e in tqdm(range(epochs, epochs+100)):\n",
    "\n",
    "    model.train()\n",
    "    train_loss, model, optimizer = pipeline_train(model, train_loader, device, optimizer,\n",
    "                                                      \"train\")\n",
    "\n",
    "    loss_train_mean = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    val_loss, model, optimizer = pipeline_train(model, valid_loader, device, optimizer,\n",
    "                                                    \"val\")\n",
    "    #scheduler.step(val_loss)\n",
    "    loss_val_mean = val_loss / len(valid_loader)\n",
    "\n",
    "    print(f'Epoch {e} \\t Training Loss: {loss_train_mean} \\t Validation Loss: {loss_val_mean}')\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, \"autoencoder.ckpt\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(x.to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
