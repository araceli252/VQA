{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base(name_arq):\n",
    "\n",
    "    f = open(name_arq, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Pega apenas as \"respondiveis\"\n",
    "    #data = [d for d in data if d[\"answerable\"] == 1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e371c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "transform_patches = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize(size=(112,112)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            #nn.Linear(2*16*c_hid, latent_dim)\n",
    "            nn.Linear(2*196*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "#################################################################################################\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            nn.Linear(latent_dim, 2*196*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        #x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = x.reshape(x.shape[0], -1, 14, 14)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "##################################################################################################\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int = 112,\n",
    "                 latent_dim: int = 768,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3,\n",
    "                 width: int = 112,\n",
    "                 height: int = 112):\n",
    "        super().__init__()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "checkpoint = torch.load(\"autoencoder.ckpt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_autoencoder(model, img_tensor):\n",
    "\n",
    "    model.eval()\n",
    "    x = model.encoder(img_tensor)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visão\n",
    "\"\"\"\n",
    "{\"imagem.jpg\": [vetores],\n",
    "\"imagem2.jpg\": [vetores],\n",
    "...\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee418f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images(base, num):\n",
    "    return torch.stack([base[i] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d57589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_visao(name_arq, model, tam_base):\n",
    "\n",
    "    # Realiza a leitura da base\n",
    "    data = load_base(name_arq)\n",
    "\n",
    "    data = data[18000:]\n",
    "\n",
    "    # Define diretório onde se encontram as imagens\n",
    "    dir_img = name_arq.split(\".json\")[0]\n",
    "\n",
    "    # Irá carregar as informações visuais referentes a cada uma das imagens\n",
    "    info_visao = {}\n",
    "    base = []\n",
    "\n",
    "    for info in tqdm(data):\n",
    "\n",
    "        # Pega o nome da imagem\n",
    "        name_img = info[\"image\"]\n",
    "\n",
    "        # Faz a leitura da imagem\n",
    "        img = Image.open(dir_img+\"/\"+name_img)\n",
    "\n",
    "        # Padroniza a imagem\n",
    "        img_patches = transform_patches(img)\n",
    "        img_tensor = transform_img(img)\n",
    "\n",
    "        # Coloca a imagem e seus patches em uma lista para obter os seus embeddings\n",
    "        base.append(img_tensor)\n",
    "        base.append(img_patches[:, :112, :112])\n",
    "        base.append(img_patches[:, :112, 112:])\n",
    "        base.append(img_patches[:, 112:, :112])\n",
    "        base.append(img_patches[:, 112:, 112:])\n",
    "\n",
    "        images = get_train_images(base, 5)\n",
    "\n",
    "        # Pega os embeddings referentes a imagem\n",
    "        embedding_img = get_embedding_autoencoder(model, images)\n",
    "\n",
    "        # Atualiza as informações da imagem\n",
    "        info_visao[name_img] = embedding_img\n",
    "\n",
    "    return info_visao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_visao(name_arq_in, info_visao):\n",
    "\n",
    "    name_arq_out = name_arq_in.split(\".json\")[0]+\"_info_visao.pkl\"\n",
    "\n",
    "    file = open(name_arq_out, 'ab')\n",
    "    pickle.dump(info_visao, file, pickle.HIGHEST_PROTOCOL)\n",
    "    file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name_arq = \"train.json\"\n",
    "info_visao = get_info_visao(name_arq, model, 1)\n",
    "save_info_visao(name_arq, info_visao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b5c03",
   "metadata": {},
   "source": [
    "#### Fontes\n",
    "\n",
    "https://colab.research.google.com/drive/1jIflL9-gktbXq_2cEE_KM7yYq2PaOKRM?authuser=1#scrollTo=8cRyNhMQaAyh\n",
    "\n",
    "https://khvmaths.medium.com/vision-transformer-understanding-the-underlying-concept-83d699d71180"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
