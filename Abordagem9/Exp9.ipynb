{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "<!-- Center the following image: -->\n",
    "<p align=\"center\">\n",
    "  <img src=\"Images/vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- **Note:** visit the [GitHub Repo](https://github.com/yousefkotp/Visual-Question-Answering/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Visual Question Answering](#toc1_)    \n",
    "  - [Installing Required Libraries](#toc1_1_)    \n",
    "  - [Importing Libraries](#toc1_2_)    \n",
    "  - [Configuring the Notebook](#toc1_3_)    \n",
    "  - [Processing Data](#toc1_4_)    \n",
    "  - [Creating Dataframes & Splitting](#toc1_5_)    \n",
    "  - [Exploratory Data Analysis](#toc1_6_)    \n",
    "    - [Training Dataframe](#toc1_6_1_)    \n",
    "    - [Validation Dataframe](#toc1_6_2_)    \n",
    "    - [Testing Dataframe](#toc1_6_3_)    \n",
    "  - [Processing Images & Questions using CLIP model](#toc1_7_)    \n",
    "  - [Creating Dataset Class](#toc1_8_)    \n",
    "  - [Building Model's Architecture](#toc1_9_)    \n",
    "  - [Loading Preprocessed Embeddings](#toc1_10_)    \n",
    "  - [Preparing Data Loaders](#toc1_11_)    \n",
    "  - [Training the Model](#toc1_12_)    \n",
    "  - [Remarks](#toc1_13_)    \n",
    "  - [Test your own image !](#toc1_14_)    \n",
    "  - [Building Test Answers](#toc1_15_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to make sure to install the required libraries. We will use [PyTorch](https://pytorch.org/) for building our model. We will also [Open AI's CLIP](https://openai.com/research/clip) pretrained model for image and text embedding which is open sourced on [GitHub](https://github.com/openai/CLIP). We will use [LaTeX](https://www.latex-project.org/) for writing our research [report](https://github.com/yousefkotp/Visual-Question-Answering/blob/9c27560e9c19a0981343fd5fce25861236ab939f/LaTeX_Paper/Visual_Question_Answering_Report.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2023-06-22T22:07:22.997395Z",
     "shell.execute_reply": "2023-06-22T22:07:22.995390Z",
     "shell.execute_reply.started": "2023-06-22T22:07:05.284610Z"
    }
   },
   "source": [
    "%pip install ftfy regex tqdm --user\n",
    "%pip install pandas --user\n",
    "%pip install wordcloud --user\n",
    "%pip install sklearn --user\n",
    "%pip install scikit-learn --user\n",
    "%pip install Levenshtein --user\n",
    "%pip install git+https://github.com/openai/CLIP.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importing Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:23.000535Z",
     "iopub.status.busy": "2023-06-22T22:07:23.000012Z",
     "iopub.status.idle": "2023-06-22T22:07:27.425570Z",
     "shell.execute_reply": "2023-06-22T22:07:27.424438Z",
     "shell.execute_reply.started": "2023-06-22T22:07:23.000486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing os, numpy and pandas for data manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization, we will use matplotlib, wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For data preprocessing, we will use Counter, train_test_split, Levenshtein distance, Python Image Library and OneHotEncoder\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torchvision.models as models_torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For saving and loading the preprocessed data, we will use pickle\n",
    "import pickle\n",
    "\n",
    "# For Building the model, we will use PyTorch and its functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# For taking the image from the URL, we will use requests\n",
    "import requests\n",
    "\n",
    "# For evaluation, we will need sklearn.metrics.average_precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Importing json for results formatting which will be uploaded for evaluation\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.427916Z",
     "iopub.status.busy": "2023-06-22T22:07:27.427181Z",
     "iopub.status.idle": "2023-06-22T22:07:27.436353Z",
     "shell.execute_reply": "2023-06-22T22:07:27.435229Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.427876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuring the paths for the dataset\n",
    "#INPUT_PATH = '/kaggle/input/vizwiz'\n",
    "#ANNOTATIONS = INPUT_PATH + '/Annotations/Annotations'\n",
    "TRAIN_PATH = 'train'\n",
    "VALIDATION_PATH = 'val'\n",
    "ANNOTATIONS_TRAIN_PATH = 'train.json'\n",
    "ANNOTATIONS_VAL_PATH = 'val.json'\n",
    "OUTPUT_PATH = 'saida_bert_resnet/'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "# Using accelerated computing if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines functions for exploratory data analysis and preprocessing of data\n",
    "- `read_dataframe` function reads a JSON file and returns a dataframe with required columns\n",
    "- `split_train_test` function splits the dataframe into train and test sets\n",
    "- `plot_histogram`, `plot_pie`, and `plot_wordcloud` functions plot the histogram, pie chart, and wordcloud of the given column, respectively\n",
    "- `explore_dataframe` function explores the dataframe by utilizing the previous functions\n",
    "- `get_number_of_distinct_answers` function returns the number of distinct answers in the dataframe\n",
    "- `process_images` function processes the images in the dataframe and returns the image features using Open AI's CLIP model\n",
    "- `process_questions` function processes the questions in the dataframe and returns the question features using Open AI's CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.440224Z",
     "iopub.status.busy": "2023-06-22T22:07:27.439403Z",
     "iopub.status.idle": "2023-06-22T22:07:27.464569Z",
     "shell.execute_reply": "2023-06-22T22:07:27.463512Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.440178Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and returns a dataframe with the required columns (image, question, answers, answer_type, answerable)\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Dataframe with the required columns\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "    df = df[['image', 'question', 'answers', 'answer_type', 'answerable']]\n",
    "    return df\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be split\n",
    "\n",
    "    Returns:\n",
    "        train (pandas.DataFrame): Train set\n",
    "        test (pandas.DataFrame): Test set\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "def plot_histogram(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the histogram of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.hist(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pie(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the pie chart of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    value_counts = dataframe[column].value_counts()\n",
    "    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the wordcloud of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in dataframe[column]])\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color ='white',\n",
    "                    min_font_size = 10).generate(text)\n",
    "\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "\n",
    "def explore_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Explores the dataframe (EDA) by plotting the pie charts, histograms and wordclouds of the columns\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plot_pie(dataframe, 'answer_type')\n",
    "    plot_pie(dataframe, 'answerable')\n",
    "    plot_histogram(dataframe, 'answerable')\n",
    "    plot_wordcloud(dataframe, 'question')\n",
    "\n",
    "def get_number_of_distinct_answers(dataframe):\n",
    "    \"\"\"\n",
    "    Returns the number of distinct answers in the dataframe\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        len(unique_answers_set) (int): Number of distinct answers in the dataframe\n",
    "    \"\"\"\n",
    "    unique_answers_set = set()\n",
    "    for row in dataframe['answers']:\n",
    "        for answer_map in row:\n",
    "            unique_answers_set.add(answer_map['answer'])\n",
    "    return len(unique_answers_set)\n",
    "\n",
    "def process_images(dataframe, image_path, model_visao, transform):\n",
    "    \"\"\"\n",
    "    Processes the images in the dataframe and returns the image features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the images\n",
    "        image_path (str): Path to the input images\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        preprocessor (clip.model.Preprocess): Preprocessor for the CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        images (list): List of image features\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        full_path = image_path + \"/\" + row['image']\n",
    "        image = Image.open(full_path)\n",
    "        # Pré-processar a imagem\n",
    "        img = transform(image).unsqueeze(0)  # Adicionar dimensão do lote (batch) ao tensor\n",
    "        # Extrair características da imagem usando a ResNet-152\n",
    "        with torch.no_grad():\n",
    "            image_features = model_visao(img)\n",
    "\n",
    "        images.append(image_features[0])\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_embedding_bert(text, model, tokenizer):\n",
    "\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on\n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "        # becase we set `output_hidden_states = True`, the third item will be the\n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    tokenized_text = tokenized_text[1:-1]\n",
    "    token_vecs_sum = token_vecs_sum[1:-1]\n",
    "    vecs_words = []\n",
    "\n",
    "    count = 0\n",
    "    pos_ini = -1\n",
    "\n",
    "    for i in range(len(token_vecs_sum)):\n",
    "\n",
    "        if \"#\" in tokenized_text[i]:\n",
    "            count += 1\n",
    "            if pos_ini == -1:\n",
    "                pos_ini = i\n",
    "\n",
    "            if i+1 == len(tokenized_text):\n",
    "                vecs_words[-1] = torch.mean(torch.stack([token_vecs_sum[j] for j in range(pos_ini-1, pos_ini+count)], dim=0), 0)\n",
    "                count = 0\n",
    "                pos_ini = -1\n",
    "\n",
    "            elif \"#\" not in tokenized_text[i+1]:\n",
    "                vecs_words[-1] = torch.mean(torch.stack([token_vecs_sum[j] for j in range(pos_ini-1, pos_ini+count)], dim=0), 0)\n",
    "                count = 0\n",
    "                pos_ini = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = 0\n",
    "            pos_ini = -1\n",
    "            vecs_words.append(token_vecs_sum[i])\n",
    "\n",
    "    embed_word = torch.stack(vecs_words, dim=0)\n",
    "    #embed_word = embed_word.numpy()\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #sentence_embedding = sentence_embedding.numpy()\n",
    "\n",
    "    #final_embedding = np.concatenate((sentence_embedding.reshape((1, sentence_embedding.shape[0])), embed_word))\n",
    "    final_embedding = torch.concatenate((sentence_embedding.reshape((1, sentence_embedding.shape[0])), embed_word))\n",
    "\n",
    "    return final_embedding\n",
    "\n",
    "def process_questions(dataframe, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Processes the questions in the dataframe and returns the question features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the questions\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        questions (list): List of question features\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['question']\n",
    "        text_features = get_embedding_bert(question, model, tokenizer)\n",
    "        questions.append(text_features)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creating Dataframes & Splitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use previously defined functions to create dataframes and split them into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.466717Z",
     "iopub.status.busy": "2023-06-22T22:07:27.466313Z",
     "iopub.status.idle": "2023-06-22T22:07:28.645746Z",
     "shell.execute_reply": "2023-06-22T22:07:28.644483Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.466672Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "train_df, test_df = split_train_test(train_df, test_size=0.05)\n",
    "ANSWER_SPACE = get_number_of_distinct_answers(train_df) # The answer space will be decreased later when we process the answers\n",
    "print(\"Number of distinct answers: \", ANSWER_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Exploratory Data Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[Training Dataframe](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:28.647592Z",
     "iopub.status.busy": "2023-06-22T22:07:28.647220Z",
     "iopub.status.idle": "2023-06-22T22:07:31.184355Z",
     "shell.execute_reply": "2023-06-22T22:07:31.183293Z",
     "shell.execute_reply.started": "2023-06-22T22:07:28.647562Z"
    }
   },
   "source": [
    "explore_dataframe(train_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(train_df))\n",
    "print(\"Number of samples in train: \", len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_2_'></a>[Validation Dataframe](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:31.650283Z",
     "iopub.status.busy": "2023-06-22T22:07:31.649889Z",
     "iopub.status.idle": "2023-06-22T22:07:33.842255Z",
     "shell.execute_reply": "2023-06-22T22:07:33.841242Z",
     "shell.execute_reply.started": "2023-06-22T22:07:31.650237Z"
    }
   },
   "source": [
    "explore_dataframe(validation_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(validation_df))\n",
    "print(\"Number of samples in validation set: \", len(validation_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_3_'></a>[Testing Dataframe](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:35.927758Z",
     "iopub.status.busy": "2023-06-22T22:07:35.927195Z",
     "iopub.status.idle": "2023-06-22T22:07:38.696563Z",
     "shell.execute_reply": "2023-06-22T22:07:38.695531Z",
     "shell.execute_reply.started": "2023-06-22T22:07:35.927711Z"
    }
   },
   "source": [
    "explore_dataframe(test_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(test_df))\n",
    "print(\"Number of samples in test: \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Processing Images & Questions using CLIP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of lazy processing of images and questions embeddings and recomputing them over and over during forward passes in the model, we can preprocess them and save them in a file using Pickle. This will save us a lot of time when we want to train our model and decrease the time taken by one epoch drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global tokenizer_bert, bert_model\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                )\n",
    "\n",
    "# Preprocessamento da imagem\n",
    "global transform, model_visao\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Redimensionar a imagem para 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Recortar o centro da imagem para 224x224 pixels\n",
    "    transforms.ToTensor(),  # Converter a imagem PIL em um tensor PyTorch\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalização com médias e desvios padrão da ImageNet\n",
    "])\n",
    "\n",
    "model_visao = models_torch.resnet152(pretrained=True)\n",
    "model_visao.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#############\n",
    "\n",
    "\n",
    "#training_images = process_images(train_df, TRAIN_PATH, clip_model, preprocessor, DEVICE)\n",
    "training_images = process_images(train_df, TRAIN_PATH, model_visao, transform)\n",
    "#training_questions = process_questions(train_df, clip_model, DEVICE)\n",
    "training_questions = process_questions(train_df, bert_model, tokenizer_bert)\n",
    "with open(OUTPUT_PATH + 'training_images.pkl', 'wb') as f:\n",
    "    pickle.dump(training_images, f)\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(training_questions, f)\n",
    "\n",
    "#validation_images = process_images(validation_df, VALIDATION_PATH, clip_model, preprocessor, DEVICE)\n",
    "validation_images = process_images(validation_df, VALIDATION_PATH, model_visao, transform)\n",
    "#validation_questions = process_questions(validation_df, clip_model, DEVICE)\n",
    "validation_questions = process_questions(validation_df, bert_model, tokenizer_bert)\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_images, f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_questions, f)\n",
    "\n",
    "#test_images = process_images(test_df, TRAIN_PATH, clip_model, preprocessor, DEVICE)\n",
    "test_images = process_images(test_df, TRAIN_PATH, model_visao, transform)\n",
    "#test_questions = process_questions(test_df, clip_model, DEVICE)\n",
    "test_questions = process_questions(test_df, bert_model, tokenizer_bert)\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'wb') as f:\n",
    "    pickle.dump(test_images, f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(test_questions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#83m 28.9s execução da célula acima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Creating Dataset Class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch requires using Dataset class. We will create a class that will be used to load the data and process it during training. We will also use this class to load the preprocessed images and questions embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:44.404295Z",
     "iopub.status.busy": "2023-06-22T22:07:44.403870Z",
     "iopub.status.idle": "2023-06-22T22:07:44.448757Z",
     "shell.execute_reply": "2023-06-22T22:07:44.447590Z",
     "shell.execute_reply.started": "2023-06-22T22:07:44.404246Z"
    }
   },
   "outputs": [],
   "source": [
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, dataframe, answer_type_onehotencoder = None, answer_onehotencoder = None, model_name = \"RN50x64\", images_features = torch.tensor([]), questions_features = torch.tensor([])):\n",
    "        super(VizWizDataset, self).__init__()\n",
    "\n",
    "        # Total counter for all answers before filtering, used in Tie Breaking when building the answer vocabulary\n",
    "        self.answer_counter = Counter()\n",
    "\n",
    "        questions_features = [questions_features[i][0] for i in range(len(questions_features))]\n",
    "\n",
    "        # Saving image & question embeddings\n",
    "        self.images_features = images_features\n",
    "        self.questions_features = questions_features\n",
    "        self.answerable = dataframe['answerable'].to_numpy()\n",
    "\n",
    "        # Saving the dataframe\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        # List for answers for each question (each question has 10 answers)\n",
    "        self.answer_counter_per_question = []\n",
    "\n",
    "        # Populating the counter for words in answers which will be used when building answer vocabulary\n",
    "        self.build_answer_counter()\n",
    "\n",
    "        # Building the answer vocabulary according to the methodology explained in the paper\n",
    "        self.build_answer_vocab()\n",
    "\n",
    "        # The number of vocabulary words after filtering\n",
    "        print(\"Number of distinct answers: \", len(self.get_answer_vocab()))\n",
    "\n",
    "        # One hot encoding the answers\n",
    "        if answer_type_onehotencoder is None:\n",
    "            answer_type_onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            answer_type_onehotencoder.fit(self.copied_dataframe[['answer_type']])\n",
    "\n",
    "        # One hot encoding the answer types\n",
    "        if answer_onehotencoder is None:\n",
    "            answer_onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            answer_onehotencoder.fit(self.copied_dataframe[['answer']])\n",
    "\n",
    "        # Saving the one hot encoders\n",
    "        self.answer_onehotencoder = answer_onehotencoder\n",
    "        self.answer_type_onehotencoder = answer_type_onehotencoder\n",
    "\n",
    "        # Transforming the answers and answer types to one hot encoded vectors\n",
    "        self.answer_onehotencoded = answer_onehotencoder.transform(self.copied_dataframe[['answer']]).toarray()\n",
    "        self.answer_type_onehotencoded = answer_type_onehotencoder.transform(self.copied_dataframe[['answer_type']]).toarray()\n",
    "\n",
    "        # Saving the answer categories (vocabulary) which will be used when getting index of the predicted answer\n",
    "        self.answers_categories = self.answer_onehotencoder.categories_[0].tolist()\n",
    "\n",
    "        # Saving answers for each question (each question has 10 answers)\n",
    "        self.build_answer_counter_per_question()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        answer = torch.tensor(self.answer_onehotencoded[index], dtype=torch.float32)\n",
    "        answer_type = torch.tensor(self.answer_type_onehotencoded[index], dtype=torch.float32)\n",
    "        answer_counter = torch.tensor(self.answer_counter_per_question[index], dtype=torch.long)\n",
    "        answerable = torch.tensor(self.answerable[index], dtype=torch.float32)\n",
    "        return self.images_features[index], self.questions_features[index], answer, answer_type, answer_counter, answerable\n",
    "\n",
    "    def build_answer_counter_per_question(self):\n",
    "\n",
    "        for index, row in self.dataframe.iterrows():\n",
    "            temp_list = []\n",
    "            for answer_map in row['answers']:\n",
    "                answer = answer_map['answer']\n",
    "                # check if answer in self.answers_categories\n",
    "                if answer in self.answers_categories:\n",
    "                    answer_index = self.answers_categories.index(answer)\n",
    "                    temp_list.append(answer_index)\n",
    "            # Torch.tensor requires the all the lists to have constant length, so we pad the list with -1 if needed\n",
    "            while len(temp_list) < 10:\n",
    "                temp_list.append(-1)\n",
    "            self.answer_counter_per_question.append(temp_list)\n",
    "\n",
    "\n",
    "\n",
    "    def build_answer_vocab(self):\n",
    "        # Building answer vocab follow this policy:\n",
    "        # for each question we have 10 answers, we choose the most frequent answer as the answer for this question\n",
    "        # if there is a tie, we choose the most common one in the whole dataset\n",
    "        # if there is a tie, we choose the pairwise Levenshtein distance is used to find the answer that is most representative to all others.\n",
    "\n",
    "        # Copying the original dataframe which will be manipulated\n",
    "        self.copied_dataframe = self.dataframe.copy()\n",
    "        self.copied_dataframe.drop(columns=['answers'], inplace=True)\n",
    "\n",
    "        # Adding extra column named 'answer'\n",
    "        self.copied_dataframe['answer'] = None\n",
    "\n",
    "        for index, row in self.dataframe.iterrows():\n",
    "            intermediate_counter = Counter()\n",
    "            for answer_map in row['answers']:\n",
    "                answer = answer_map['answer']\n",
    "                intermediate_counter.update([answer])\n",
    "\n",
    "            # let's see the top elements in the answers_counter to check if there is a tie\n",
    "            top_elements = intermediate_counter.most_common(1)\n",
    "            if len(top_elements) == 1:\n",
    "                self.copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "            else:\n",
    "                # let's see who is the most common answer in the whole dataset\n",
    "                top_elements = self.answer_counter.most_common(1)\n",
    "                if len(top_elements) == 1:\n",
    "                    self.copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "                else:\n",
    "                    # let's get the minimum levenshtein distance between the answers in top_elements\n",
    "                    current_min = np.inf\n",
    "                    current_answer = None\n",
    "                    for answer in top_elements:\n",
    "                        total_distance = 0\n",
    "                        for answer2 in top_elements:\n",
    "                            if answer != answer2:\n",
    "                                lev_distance = lev.distance(answer[0], answer2[0])\n",
    "                                total_distance += lev_distance\n",
    "                        if total_distance < current_min:\n",
    "                            current_min = total_distance\n",
    "                            current_answer = answer[0]\n",
    "                    self.copied_dataframe.at[index, 'answer'] = current_answer\n",
    "        return\n",
    "\n",
    "    def build_answer_counter(self):\n",
    "        for row in self.dataframe['answers']:\n",
    "            for answer_map in row:\n",
    "                self.answer_counter.update([answer_map['answer']])\n",
    "\n",
    "    def get_answer_vocab(self):\n",
    "        return self.copied_dataframe['answer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Building Model's Architecture](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our model's architecture according to the paper. We will use PyTorch to build our model as we said before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:08:52.934991Z",
     "iopub.status.busy": "2023-06-22T22:08:52.934623Z",
     "iopub.status.idle": "2023-06-22T22:08:53.005992Z",
     "shell.execute_reply": "2023-06-22T22:08:53.004837Z",
     "shell.execute_reply.started": "2023-06-22T22:08:52.934962Z"
    }
   },
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, hidden_size, model_name = \"ViT-L/14@336px\", device = torch.device(\"cpu\")):\n",
    "        super(VQAModel, self).__init__()\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "\n",
    "        self.vizwiz_training_accuracies = []\n",
    "        self.vizwiz_validation_accuracies = []\n",
    "\n",
    "        self.training_answerability = []\n",
    "        self.validation_answerability = []\n",
    "\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Initializing Binary Cross Entropy Loss which will be used to train the model on answerability\n",
    "        self.answerability_loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Loading the CLIP model\n",
    "        #self.clip_model, self.preprocess = clip.load(model_name, device = device)\n",
    "\n",
    "        # Freezing the CLIP model\n",
    "        #for param in self.clip_model.parameters():\n",
    "        #    param.requires_grad = False\n",
    "\n",
    "        # First linear layer\n",
    "        self.linear_layer1 = nn.Sequential(\n",
    "            #nn.LayerNorm(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1]),\n",
    "            nn.LayerNorm(1768),\n",
    "            nn.Dropout(p=0.5),\n",
    "            #nn.Linear(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1], hidden_size)\n",
    "            nn.Linear(1768, hidden_size)\n",
    "\n",
    "        )\n",
    "\n",
    "        # Second linear layer\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "        self.answer_type_layer = nn.Linear(hidden_size, 4)\n",
    "        self.answer_mask_layer = nn.Linear(4, num_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Answerability Linear Layer (We removed drop out layer because training answerability was very bad)\n",
    "        self.answerability_linear_layer = nn.Sequential(\n",
    "            #nn.LayerNorm(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1]),\n",
    "            nn.LayerNorm(1768),\n",
    "            #nn.Linear(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1], hidden_size)\n",
    "            nn.Linear(1768, hidden_size)\n",
    "        )\n",
    "\n",
    "        # Answerability Sigmoid Layer\n",
    "        self.answerability_final_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Sigmoid Layer for Answerability\n",
    "        self.answerability_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image, question):\n",
    "\n",
    "        # Flattening and concatenating the image and question features\n",
    "        image = torch.flatten(image, start_dim=1)\n",
    "        question = torch.flatten(question, start_dim=1)\n",
    "        features = torch.cat((image, question), dim=1)\n",
    "\n",
    "        # Calculating the answerability score\n",
    "        answerability_score = self.answerability_linear_layer(features)\n",
    "        answerability_score = self.answerability_final_layer(answerability_score)\n",
    "        answerability_score = self.answerability_sigmoid(answerability_score)\n",
    "        answerability_score = answerability_score.squeeze()\n",
    "\n",
    "        # Passing the features through the first linear layer\n",
    "        features = self.linear_layer1(features)\n",
    "\n",
    "        # Passing the features to get 4 answer types\n",
    "        answer_type = self.answer_type_layer(features)\n",
    "\n",
    "        # Expanding answer make to the same size as the number of classes (vocab size)\n",
    "        answer_mask = self.answer_mask_layer(answer_type)\n",
    "\n",
    "        # Applying sigmoid to get the answer mask\n",
    "        answer_mask = self.sigmoid(answer_mask)\n",
    "\n",
    "        # Passing the features through the second linear layer\n",
    "        output = self.linear_layer2(features)\n",
    "\n",
    "        # Applying the answer mask to the output\n",
    "        output = output * answer_mask\n",
    "\n",
    "        return output, answer_type, answerability_score\n",
    "\n",
    "    def train_model(self, training_dataloader, validation_dataloader, test_dataloader, criterion, optimizer, epochs = 10, save_path = None, save_every = 1):\n",
    "        for epoch in range(1,epochs+1):\n",
    "            training_loss, training_accuracy, training_vizwiz_accuracy, train_answerability_score = self.training_step(training_dataloader, criterion, optimizer, self.device)\n",
    "            validation_loss, validation_accuracy, validation_vizwiz_accuracy, validation_answerability_score = self.validation_step(validation_dataloader, criterion, self.device)\n",
    "            test_accuracy, test_vizwiz_accuracy, test_answerability_score = self.test_step(test_dataloader)\n",
    "\n",
    "            self.training_losses.append(training_loss)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "            self.training_accuracies.append(training_accuracy)\n",
    "            self.validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "            self.vizwiz_training_accuracies.append(training_vizwiz_accuracy)\n",
    "            self.vizwiz_validation_accuracies.append(validation_vizwiz_accuracy)\n",
    "\n",
    "            self.training_answerability.append(train_answerability_score)\n",
    "            self.validation_answerability.append(validation_answerability_score)\n",
    "\n",
    "\n",
    "            print(\"Epoch: {} | Training Loss: {:.3f} | Validation Loss: {:.3f}\".format(epoch, training_loss, validation_loss))\n",
    "            print(\"Epoch: {} | Training Accuracy: {:.3f} | Validation Accuracy: {:.3f} | Test Accuracy: {:.3f}\".format(epoch, training_accuracy, validation_accuracy, test_accuracy))\n",
    "            print(\"Epoch: {} | Training VizWiz Accuracy: {:.3f} | Validation VizWiz Accuracy: {:.3f} | Test VizWiz Accuracy: {:.3f}\".format(epoch, training_vizwiz_accuracy, validation_vizwiz_accuracy, test_vizwiz_accuracy))\n",
    "            print(\"Epoch: {} | Training Answerability Score: {:.3f} | Validation Answerability Score: {:.3f} | Test Answerability Score: {:.3f}\\n\".format(epoch, train_answerability_score, validation_answerability_score, test_answerability_score))\n",
    "\n",
    "            if save_path != None and epoch % save_every == 0:\n",
    "                self.save_model(save_path + \"epoch_{}.pth\".format(epoch))\n",
    "        return\n",
    "\n",
    "    def training_step(self, dataloader, criterion, optimizer, device):\n",
    "        training_loss, training_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        self.train()\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "\n",
    "            image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "            answerable = 1 - answerable\n",
    "            answerable_predict = 1.0 - answerable_predict\n",
    "            loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "            predicted_answer = torch.argmax(output, dim = 1)\n",
    "            actual_answer = torch.argmax(answer, dim = 1)\n",
    "            for i in range(len(answer)):\n",
    "                if actual_answer[i] == predicted_answer[i]:\n",
    "                    training_accuracy +=1\n",
    "                total_sum +=1\n",
    "                vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                answerable_true.append(answerable[i].item())\n",
    "                answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        training_loss /= len(dataloader)\n",
    "        training_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        return training_loss, training_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "\n",
    "    def validation_step(self, dataloader, criterion, device):\n",
    "        validation_loss, validation_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "                output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "\n",
    "                # Answerablity is the confidence that quesion is not answerable, so we have to subtract from 1\n",
    "                answerable = 1 - answerable\n",
    "                answerable_predict = 1.0 - answerable_predict\n",
    "                loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "                validation_loss += loss.item()\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                    if torch.sum(answer[i]) == 0:\n",
    "                        continue\n",
    "                    if actual_answer[i] == predicted_answer[i]:\n",
    "                        validation_accuracy += 1\n",
    "                    total_sum +=1\n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    answerable_true.append(answerable[i].item())\n",
    "                    answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        validation_loss /= len(dataloader)\n",
    "        validation_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        # We will use weighted average since that there is imbalance in answerability in the dataset as displayed in EDA section\n",
    "        return validation_loss, validation_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "    def test_step(self, dataloader):\n",
    "        self.eval()\n",
    "        accuracy, total_sum, vizwiz_accuracy = 0.0, 0, 0.0\n",
    "        answerable_true = []\n",
    "        answerable_predicted = []\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = image.to(self.device), question.to(self.device), answer.to(self.device), answer_type.to(self.device), answers_for_questions.to(self.device), answerable.to(self.device)\n",
    "                output, _, answerable_predict = self.forward(image, question)\n",
    "                answerable = 1 - answerable\n",
    "                answerable_predict = 1.0 - answerable_predict\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                    if torch.sum(answer[i]) == 0:\n",
    "                        continue\n",
    "                    if predicted_answer[i] == actual_answer[i]:\n",
    "                        accuracy += 1\n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    total_sum +=1\n",
    "                    answerable_true.append(answerable[i].item())\n",
    "                    answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        answerable_true = np.array(answerable_true)\n",
    "        answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "        return accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model state dictionary to the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to save the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model state dictionary from the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to load the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - self: the loaded model object\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        return self\n",
    "\n",
    "    def predict(self, image, question):\n",
    "        \"\"\"\n",
    "        Predicts the output and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image (tensor): the image tensor\n",
    "        - question (tensor): the question tensor\n",
    "\n",
    "        Returns:\n",
    "        - output (tensor): the predicted output tensor\n",
    "        - answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        output, answer_type, answerability = self.forward(image, question)\n",
    "        answerability = 1.0 - answerability\n",
    "        return output, answer_type, answerability\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation losses.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_losses, label = \"Training Loss\")\n",
    "        plt.plot(self.validation_losses, label = \"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation accuracies.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_accuracies, label = \"Training Accuracy\")\n",
    "        plt.plot(self.validation_accuracies, label = \"Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_vizwiz_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plots the VizWiz training and validation accuracies.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.vizwiz_training_accuracies, label = \"VizWiz Training Accuracy\")\n",
    "        plt.plot(self.vizwiz_validation_accuracies, label = \"VizWiz Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_answerability(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation answerabilities.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_answerability, label = \"Training Answerability\")\n",
    "        plt.plot(self.validation_answerability, label = \"Validation Answerability\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def test_model(self, image_path, question):\n",
    "        \"\"\"\n",
    "        Tests the model by predicting the answer and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image_path (str): the path to the image file or URL\n",
    "        - question (str): the question to be asked\n",
    "\n",
    "        Returns:\n",
    "        - predicted_answer (tensor): the predicted answer tensor\n",
    "        - predicted_answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        if image_path.startswith(\"http\"):\n",
    "            image = Image.open(requests.get(image_path, stream = True).raw)\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "        # Pré-processar a imagem\n",
    "        img = transform(image).unsqueeze(0)  # Adicionar dimensão do lote (batch) ao tensor\n",
    "\n",
    "        # Extrair características da imagem usando a ResNet-152\n",
    "        with torch.no_grad():\n",
    "            image_features = model_visao(img)\n",
    "\n",
    "        image_features = image_features.cuda()\n",
    "\n",
    "        text_features = get_embedding_bert(question, bert_model, tokenizer_bert)\n",
    "        text_features = text_features[0].reshape((1,768))\n",
    "        text_features  = text_features.cuda()\n",
    "\n",
    "        #question =  clip.tokenize(question).to(self.device)\n",
    "        #text_features = self.clip_model.encode_text(question).float()\n",
    "        #text_features = torch.flatten(text_features, start_dim=1)\n",
    "\n",
    "        predicted_answer, predicted_answer_type, answerability = self.predict(image_features, text_features)\n",
    "        return predicted_answer, predicted_answer_type, answerability\n",
    "\n",
    "    def print_CLIP_model(self):\n",
    "        \"\"\"\n",
    "        Prints the details of the selected CLIP model.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        input_resolution = self.clip_model.visual.input_resolution\n",
    "        context_length = self.clip_model.context_length\n",
    "        vocab_size = self.clip_model.vocab_size\n",
    "\n",
    "        print(\"Selected model:\", self.model_name)\n",
    "        print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in self.clip_model.parameters()]):,}\")\n",
    "        print(\"Input resolution:\", input_resolution)\n",
    "        print(\"Context length:\", context_length)\n",
    "        print(\"Vocab size:\", vocab_size)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Loading Preprocessed Embeddings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:08:55.516451Z",
     "iopub.status.busy": "2023-06-22T22:08:55.516038Z",
     "iopub.status.idle": "2023-06-22T22:09:12.361279Z",
     "shell.execute_reply": "2023-06-22T22:09:12.360245Z",
     "shell.execute_reply.started": "2023-06-22T22:08:55.516418Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_PATH + 'training_images.pkl', 'rb') as f:\n",
    "    training_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'rb') as f:\n",
    "    training_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'rb') as f:\n",
    "    validation_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'rb') as f:\n",
    "    validation_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'rb') as f:\n",
    "    test_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'rb') as f:\n",
    "    test_questions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[Preparing Data Loaders](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:09:12.363829Z",
     "iopub.status.busy": "2023-06-22T22:09:12.363447Z",
     "iopub.status.idle": "2023-06-22T22:10:01.685916Z",
     "shell.execute_reply": "2023-06-22T22:10:01.684889Z",
     "shell.execute_reply.started": "2023-06-22T22:09:12.363794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constructing the training dataset\n",
    "training_dataset = VizWizDataset(train_df, None, None, MODEL_NAME, training_images, training_questions)\n",
    "ANSWER_ONEHOTENCODER = training_dataset.answer_onehotencoder\n",
    "ANSWER_TYPE_ONEHOTENCODER = training_dataset.answer_type_onehotencoder\n",
    "\n",
    "# Saving the fitted one hot encoders\n",
    "with open(OUTPUT_PATH + 'answer_onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(ANSWER_ONEHOTENCODER, f)\n",
    "with open(OUTPUT_PATH + 'answer_type_onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(ANSWER_TYPE_ONEHOTENCODER, f)\n",
    "\n",
    "# Constructing the validation dataset\n",
    "validation_dataset = VizWizDataset(validation_df, ANSWER_TYPE_ONEHOTENCODER, ANSWER_ONEHOTENCODER, MODEL_NAME, validation_images, validation_questions)\n",
    "\n",
    "# Constructing the test dataset\n",
    "test_dataset = VizWizDataset(test_df, ANSWER_TYPE_ONEHOTENCODER, ANSWER_ONEHOTENCODER, MODEL_NAME, test_images, test_questions)\n",
    "\n",
    "# Configuring the data loaders\n",
    "BATCH_SIZE = 32 # 64 is good too but 32 is better (variance wise)\n",
    "\n",
    "# Constructing the training, validation and test data loaders\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[Training the Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:11:37.201389Z",
     "iopub.status.busy": "2023-06-22T22:11:37.200975Z",
     "iopub.status.idle": "2023-06-22T22:18:26.935357Z",
     "shell.execute_reply": "2023-06-22T22:18:26.934296Z",
     "shell.execute_reply.started": "2023-06-22T22:11:37.201357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuring training's hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_CLASSES = len(training_dataset.get_answer_vocab())\n",
    "SAVE_PATH = OUTPUT_PATH\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "# Initializing the model\n",
    "model = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=750, model_name=MODEL_NAME).to(DEVICE)\n",
    "#model.print_CLIP_model()\n",
    "\n",
    "# Initializing the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "# Training the model and plotting the loss and accuracy\n",
    "model.train_model(training_dataloader, validation_dataloader, test_dataloader, loss_function, optimizer, epochs=NUM_EPOCHS, save_path=SAVE_PATH, save_every=SAVE_EVERY)\n",
    "model.plot_loss()\n",
    "model.plot_accuracy()\n",
    "model.plot_vizwiz_accuracy()\n",
    "model.plot_answerability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model at **epoch number 45** outperforms the same model at any other epochs, so let's pick this model as our ultimate and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_13_'></a>[Remarks](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, the model is very light weight and fast, it takes ~ 1 minute to run an epoch. In addition to this, the model converges very fast, it only takes a maximum of 30 epoch to fully converges. This is due to the fact that we are using CLIP model which is pretrained on a huge dataset.\n",
    "- We can further improve the model by training more models with same architecture but different backbone for CLIP model. We can also use different pretrained models for image and text embeddings and ensemble them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_14_'></a>[Test your own image !](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part of code allows the user to test his own image using the trained model. You just have to configure `IMAGE_PATH` and `QUESTION` variables and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:30:10.813107Z",
     "iopub.status.busy": "2023-06-22T22:30:10.812592Z",
     "iopub.status.idle": "2023-06-22T22:30:23.077168Z",
     "shell.execute_reply": "2023-06-22T22:30:23.075981Z",
     "shell.execute_reply.started": "2023-06-22T22:30:10.813066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taking a sample image and question from the user\n",
    "QUESTION = \"What kind of food is this?\"\n",
    "IMAGE_PATH = \"train/VizWiz_train_00000008.jpg\"\n",
    "\n",
    "# Loading the fitted One Hot Encoders from the disk\n",
    "with open(OUTPUT_PATH + 'answer_onehotencoder.pkl', 'rb') as f:\n",
    "    ANSWER_ONEHOTENCODER = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'answer_type_onehotencoder.pkl', 'rb') as f:\n",
    "    ANSWER_TYPE_ONEHOTENCODER = pickle.load(f)\n",
    "\n",
    "# Loading the model from the disk\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"ViT-L/14@336px\"\n",
    "NUM_CLASSES = len(training_dataset.get_answer_vocab())\n",
    "MODEL_PATH = \"saida_bert_resnet/epoch_50.pth\" # OUTPUT_PATH + 'model.pt'\n",
    "model = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=750, model_name=MODEL_NAME).to(DEVICE)\n",
    "model.load_model(MODEL_PATH)\n",
    "\n",
    "# Predicting the answer and answer type\n",
    "predicted_answer, predicted_answer_type, answerability = model.test_model(image_path = IMAGE_PATH, question = QUESTION)\n",
    "answer = ANSWER_ONEHOTENCODER.inverse_transform(predicted_answer.cpu().detach().numpy())\n",
    "answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "\n",
    "# Printing the answer and answer type\n",
    "print(\"The Answer is: \" + answer[0][0])\n",
    "print(\"The Answer Type is: \" + answer_type[0][0])\n",
    "print(\"The confidence for being unanswerable: \" + str(answerability.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_15_'></a>[Building Test Answers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"test.json\")\n",
    "df = df[['image', 'question']]\n",
    "\n",
    "# let's create two json objects to store the output of the model then write them to a file\n",
    "model_answers = []\n",
    "model_answerability = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    image_url = df['image'][i]\n",
    "    question = df['question'][i]\n",
    "    image_path = \"test/\" + image_url\n",
    "    predicted_answer, predicted_answer_type, answerability = model.test_model(image_path = image_path, question = question)\n",
    "    answer = ANSWER_ONEHOTENCODER.inverse_transform(predicted_answer.cpu().detach().numpy())\n",
    "    answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "    answer_result = {'image': image_url, 'answer': answer[0][0]}\n",
    "    answerability_result = {'image': image_url, 'answerability': answerability.item()}\n",
    "    model_answers.append(answer_result)\n",
    "    model_answerability.append(answerability_result)\n",
    "\n",
    "# Writing them using pickle\n",
    "with open('answers_results.json', 'w') as file:\n",
    "    json.dump(model_answers, file)\n",
    "with open('answerability_results.json', 'w') as file:\n",
    "    json.dump(model_answerability, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
