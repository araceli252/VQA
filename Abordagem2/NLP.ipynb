{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59100b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings \n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from spacy import displacy\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Inicializa modelos\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base(name_arq):\n",
    "    \n",
    "    f = open(name_arq, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # Pega apenas as \"respondiveis\"\n",
    "    #data = [d for d in data if d[\"answerable\"] == 1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(text):\n",
    "    \n",
    "    # Expand contractions like \"I'll\" to \"I will\"\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Padroniza todas as palavras para minúsculo\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove \"__\"\n",
    "    text = text.replace(\"_\", \"\")\n",
    "    \n",
    "    # Retira espaços extras\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9474d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connections(text):\n",
    "    \n",
    "    # Ligações estabelecidas via análise sintática\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    text_doc = \"\"\n",
    "    for token in doc:\n",
    "        text_doc = text_doc+\" \"+token.text\n",
    "        \n",
    "    len_text = len(text_doc.split())\n",
    "    deps_parse = displacy.parse_deps(doc)\n",
    "    \n",
    "    # Já considera a bidirecionalidade\n",
    "    ligacoes_sintatic = [[con[\"start\"], con[\"end\"]] for con in deps_parse[\"arcs\"]]+[[con[\"end\"], con[\"start\"]] for con in deps_parse[\"arcs\"]]\n",
    "    \n",
    "    ligacoes = [[]]*len_text\n",
    "    \n",
    "    for w in ligacoes_sintatic:\n",
    "        \n",
    "        if len(ligacoes[w[0]]) == 0:\n",
    "            ligacoes[w[0]] = [w[1]]\n",
    "        else:\n",
    "            ligacoes[w[0]].append(w[1])\n",
    "            \n",
    "    # Ligações de acordo com a bidirecionalidade das palavras (palavras anterior e posterior)\n",
    "    for i in range(len_text):\n",
    "    \n",
    "        if i == 0:\n",
    "            ligacoes[i].append(1)\n",
    "\n",
    "        elif i == (len_text-1):\n",
    "            ligacoes[i].append(len_text-2)\n",
    "\n",
    "        else:\n",
    "            ligacoes[i].append(i-1)\n",
    "            ligacoes[i].append(i+1)\n",
    "\n",
    "    ligacoes = [list(set(i)) for i in ligacoes]\n",
    "    \n",
    "    return len_text, text_doc.split(), ligacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_bert(text, model, tokenizer):\n",
    "    \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    \n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    \n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "    tokenized_text = tokenized_text[1:-1]\n",
    "    token_vecs_sum = token_vecs_sum[1:-1]\n",
    "    vecs_words = []\n",
    "\n",
    "    count = 0\n",
    "    pos_ini = -1\n",
    "\n",
    "    for i in range(len(token_vecs_sum)):\n",
    "\n",
    "        if \"#\" in tokenized_text[i]:\n",
    "            count += 1  \n",
    "            if pos_ini == -1:\n",
    "                pos_ini = i\n",
    "                \n",
    "            if i+1 == len(tokenized_text):\n",
    "                vecs_words[-1] = torch.mean(torch.stack([token_vecs_sum[j] for j in range(pos_ini-1, pos_ini+count)], dim=0), 0)\n",
    "                count = 0\n",
    "                pos_ini = -1   \n",
    "                \n",
    "            elif \"#\" not in tokenized_text[i+1]:\n",
    "                vecs_words[-1] = torch.mean(torch.stack([token_vecs_sum[j] for j in range(pos_ini-1, pos_ini+count)], dim=0), 0)\n",
    "                count = 0\n",
    "                pos_ini = -1  \n",
    "                \n",
    "        else:\n",
    "\n",
    "            count = 0\n",
    "            pos_ini = -1\n",
    "            vecs_words.append(token_vecs_sum[i])   \n",
    "\n",
    "    embed_word = torch.stack(vecs_words, dim=0)\n",
    "    embed_word = embed_word.numpy()\n",
    "    \n",
    "    # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    sentence_embedding = sentence_embedding.numpy()\n",
    "    \n",
    "    final_embedding = np.concatenate((sentence_embedding.reshape((1, sentence_embedding.shape[0])), embed_word))\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "\"\"\"\n",
    "{\"imagem.jpg\":{\"len_perg\": x, \"word_0\": {\"word\": word, \"ligacoes\": [1, 3]}, ...., \"word_n\": {\"word\": word, \"ligacoes\": [n-1, n-4]}, \"embedding\": [vetor]}, \n",
    "...\n",
    "\"imagem.jpg\":{\"len_perg\": x, \"word_0\": {\"word\": word, \"ligacoes\": [1, 3]}, ...., \"word_n\": {\"word\": word, \"ligacoes\": [n-1, n-4]}, \"embedding\": [vetor]}, \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58560531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_nlp(name_arq, model, tokenizer, tam_base):\n",
    "    \n",
    "    # Realiza a leitura da base\n",
    "    data = load_base(name_arq)\n",
    "    \n",
    "    #data = data[19:20]\n",
    "    \n",
    "    # Irá carregar as informações textuais referentes a cada uma das imagens\n",
    "    info_nlp = {}\n",
    "    \n",
    "    for info in tqdm(data):\n",
    "        \n",
    "        # Inicializa o dicionário referente a dada imagem\n",
    "        info_nlp[info[\"image\"]] = {}\n",
    "        \n",
    "        # Pega a pergunta e realiza pré-processamento em cima dela\n",
    "        perg = info[\"question\"]\n",
    "        perg = Preprocessing(perg)\n",
    "        \n",
    "        # Estabelece as conexões de cada palavra dentro da pergunta\n",
    "        len_text, words_list, ligacoes = get_connections(perg)\n",
    "        \n",
    "        perg = ' '.join(words_list)\n",
    "        \n",
    "        # Calcula o tamanho da pergunta, ou seja, quantidade de palavras\n",
    "        info_nlp[info[\"image\"]][\"len_perg\"] = len_text\n",
    "        \n",
    "        # Adiciona as informações calculadas até o momento no dicionário referente a imagem em análise\n",
    "        for i in range(len_text):\n",
    "            \n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)] = {}\n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)][\"word\"] = words_list[i]\n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)][\"ligacoes\"] = ligacoes[i]\n",
    "\n",
    "        # Calcula os embeddings das palavras e adiciona ao dicionário    \n",
    "        embeddings = get_embedding_bert(perg, model, tokenizer)\n",
    "        \n",
    "        info_nlp[info[\"image\"]][\"embeddings\"] = embeddings\n",
    "\n",
    "    return info_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_nlp(name_arq_in, info_nlp):\n",
    "    \n",
    "    name_arq_out = \"bases/\"+name_arq_in.split(\".json\")[0]+\"_info_nlp\"\n",
    "    \n",
    "    file = open(name_arq_out, 'wb')\n",
    "    pickle.dump(info_nlp, file)                   \n",
    "    file.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed287df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name_arq = \"val.json\"\n",
    "info_nlp = get_info_nlp(name_arq, model, tokenizer, 5)\n",
    "save_info_nlp(name_arq, info_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cc7ba",
   "metadata": {},
   "source": [
    "#### Fontes\n",
    "\n",
    "https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/\n",
    "\n",
    "https://spacy.io/api/dependencyparser\n",
    "    \n",
    "https://spacy.io/usage/visualizers\n",
    "    \n",
    "https://python.plainenglish.io/how-to-generate-word-embedding-using-bert-2b9e79c27396\n",
    "\n",
    "https://peaceful0907.medium.com/sentence-embedding-by-bert-and-sentence-similarity-759f7beccbf1\n",
    "\n",
    "https://spacy.io/api/top-level#displacy.parse_deps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
