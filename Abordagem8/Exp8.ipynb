{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "<!-- Center the following image: -->\n",
    "<p align=\"center\">\n",
    "  <img src=\"Images/vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- **Note:** visit the [GitHub Repo](https://github.com/yousefkotp/Visual-Question-Answering/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Visual Question Answering](#toc1_)    \n",
    "  - [Installing Required Libraries](#toc1_1_)    \n",
    "  - [Importing Libraries](#toc1_2_)    \n",
    "  - [Configuring the Notebook](#toc1_3_)    \n",
    "  - [Processing Data](#toc1_4_)    \n",
    "  - [Creating Dataframes & Splitting](#toc1_5_)    \n",
    "  - [Exploratory Data Analysis](#toc1_6_)    \n",
    "    - [Training Dataframe](#toc1_6_1_)    \n",
    "    - [Validation Dataframe](#toc1_6_2_)    \n",
    "    - [Testing Dataframe](#toc1_6_3_)    \n",
    "  - [Processing Images & Questions using CLIP model](#toc1_7_)    \n",
    "  - [Creating Dataset Class](#toc1_8_)    \n",
    "  - [Building Model's Architecture](#toc1_9_)    \n",
    "  - [Loading Preprocessed Embeddings](#toc1_10_)    \n",
    "  - [Preparing Data Loaders](#toc1_11_)    \n",
    "  - [Training the Model](#toc1_12_)    \n",
    "  - [Remarks](#toc1_13_)    \n",
    "  - [Test your own image !](#toc1_14_)    \n",
    "  - [Building Test Answers](#toc1_15_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to make sure to install the required libraries. We will use [PyTorch](https://pytorch.org/) for building our model. We will also [Open AI's CLIP](https://openai.com/research/clip) pretrained model for image and text embedding which is open sourced on [GitHub](https://github.com/openai/CLIP). We will use [LaTeX](https://www.latex-project.org/) for writing our research [report](https://github.com/yousefkotp/Visual-Question-Answering/blob/9c27560e9c19a0981343fd5fce25861236ab939f/LaTeX_Paper/Visual_Question_Answering_Report.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2023-06-22T22:07:22.997395Z",
     "shell.execute_reply": "2023-06-22T22:07:22.995390Z",
     "shell.execute_reply.started": "2023-06-22T22:07:05.284610Z"
    }
   },
   "source": [
    "%pip install ftfy regex tqdm --user\n",
    "%pip install pandas --user\n",
    "%pip install wordcloud --user\n",
    "%pip install sklearn --user\n",
    "%pip install scikit-learn --user\n",
    "%pip install Levenshtein --user\n",
    "%pip install git+https://github.com/openai/CLIP.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Importing Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:23.000535Z",
     "iopub.status.busy": "2023-06-22T22:07:23.000012Z",
     "iopub.status.idle": "2023-06-22T22:07:27.425570Z",
     "shell.execute_reply": "2023-06-22T22:07:27.424438Z",
     "shell.execute_reply.started": "2023-06-22T22:07:23.000486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing os, numpy and pandas for data manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization, we will use matplotlib, wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For data preprocessing, we will use Counter, train_test_split, Levenshtein distance, Python Image Library and OneHotEncoder\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For saving and loading the preprocessed data, we will use pickle\n",
    "import pickle\n",
    "\n",
    "# For Building the model, we will use PyTorch and its functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For taking the image from the URL, we will use requests\n",
    "import requests\n",
    "\n",
    "# For evaluation, we will need sklearn.metrics.average_precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Importing json for results formatting which will be uploaded for evaluation\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.427916Z",
     "iopub.status.busy": "2023-06-22T22:07:27.427181Z",
     "iopub.status.idle": "2023-06-22T22:07:27.436353Z",
     "shell.execute_reply": "2023-06-22T22:07:27.435229Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.427876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuring the paths for the dataset\n",
    "TRAIN_PATH = 'train'\n",
    "VALIDATION_PATH = 'val'\n",
    "ANNOTATIONS_TRAIN_PATH = 'train.json'\n",
    "ANNOTATIONS_VAL_PATH = 'val.json'\n",
    "OUTPUT_PATH = 'saida_clip/'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "# Using accelerated computing if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines functions for exploratory data analysis and preprocessing of data\n",
    "- `read_dataframe` function reads a JSON file and returns a dataframe with required columns\n",
    "- `split_train_test` function splits the dataframe into train and test sets\n",
    "- `plot_histogram`, `plot_pie`, and `plot_wordcloud` functions plot the histogram, pie chart, and wordcloud of the given column, respectively\n",
    "- `explore_dataframe` function explores the dataframe by utilizing the previous functions\n",
    "- `get_number_of_distinct_answers` function returns the number of distinct answers in the dataframe\n",
    "- `process_images` function processes the images in the dataframe and returns the image features using Open AI's CLIP model\n",
    "- `process_questions` function processes the questions in the dataframe and returns the question features using Open AI's CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.440224Z",
     "iopub.status.busy": "2023-06-22T22:07:27.439403Z",
     "iopub.status.idle": "2023-06-22T22:07:27.464569Z",
     "shell.execute_reply": "2023-06-22T22:07:27.463512Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.440178Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and returns a dataframe with the required columns (image, question, answers, answer_type, answerable)\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Dataframe with the required columns\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "    df = df[['image', 'question', 'answers', 'answer_type', 'answerable']]\n",
    "    return df\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be split\n",
    "\n",
    "    Returns:\n",
    "        train (pandas.DataFrame): Train set\n",
    "        test (pandas.DataFrame): Test set\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "def process_images(dataframe, image_path, clip_model, preprocessor, device):\n",
    "    \"\"\"\n",
    "    Processes the images in the dataframe and returns the image features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the images\n",
    "        image_path (str): Path to the input images\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        preprocessor (clip.model.Preprocess): Preprocessor for the CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        images (list): List of image features\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        full_path = image_path + \"/\" + row['image']\n",
    "        image = Image.open(full_path)\n",
    "        image = preprocessor(image).unsqueeze(0).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        image_features = torch.flatten(image_features, start_dim=1)\n",
    "        images.append(image_features)\n",
    "    return images\n",
    "\n",
    "def process_questions(dataframe, clip_model,device):\n",
    "    \"\"\"\n",
    "    Processes the questions in the dataframe and returns the question features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the questions\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        questions (list): List of question features\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['question']\n",
    "        question =  clip.tokenize(question).to(device)\n",
    "        text_features = clip_model.encode_text(question).float()\n",
    "        text_features = torch.flatten(text_features, start_dim=1)\n",
    "        questions.append(text_features)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creating Dataframes & Splitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use previously defined functions to create dataframes and split them into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:07:27.466717Z",
     "iopub.status.busy": "2023-06-22T22:07:27.466313Z",
     "iopub.status.idle": "2023-06-22T22:07:28.645746Z",
     "shell.execute_reply": "2023-06-22T22:07:28.644483Z",
     "shell.execute_reply.started": "2023-06-22T22:07:27.466672Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "#train_df = train_df.groupby('answer_type').apply(lambda x: x.sample(25)).reset_index(drop = True)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "#validation_df = validation_df.groupby('answer_type').apply(lambda x: x.sample(25)).reset_index(drop = True)\n",
    "train_df, test_df = split_train_test(train_df, test_size=0.05)\n",
    "train_df, test_df = train_df.reset_index(drop = True), test_df.reset_index(drop = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleciona o espaço de resposta e altera os dataframes para ficar compatível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_answers(train_df, neurons_final_layer):\n",
    "\n",
    "    df_answers = pd.DataFrame()\n",
    "    for index in range(train_df.shape[0]):\n",
    "        df_intermed = pd.DataFrame(train_df[\"answers\"][index])\n",
    "        df_answers = pd.concat([df_answers, df_intermed])\n",
    "\n",
    "    df_answers = df_answers[df_answers.answer_confidence.isin([\"yes\", \"maybe\"])].answer.value_counts().reset_index().rename(columns = {\"answer\": \"freq\", \"index\": \"answer\"})\n",
    "    df_answers = df_answers.head(neurons_final_layer)\n",
    "\n",
    "    return df_answers.answer.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space = get_most_common_answers(train_df, 3000)\n",
    "answer_space[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o codificador e decodificador das classes a ser usado na etapa de treinamento/validação\n",
    "encoder_label = {w: i for i,w in enumerate(answer_space)}\n",
    "decoder_label = {w: i for w,i in enumerate(answer_space)}\n",
    "\n",
    "with open(OUTPUT_PATH+'encoder_label.pkl', 'wb') as handle:\n",
    "    pickle.dump(encoder_label, handle)\n",
    "\n",
    "with open(OUTPUT_PATH+'decoder_label.pkl', 'wb') as handle:\n",
    "    pickle.dump(decoder_label, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answers(answers, answer_space):\n",
    "\n",
    "    df_answers = pd.DataFrame(answers)\n",
    "    common = list(set(answer_space)&set(df_answers.answer))\n",
    "    if len(common) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"proceed\"] = train_df[\"answers\"].apply(check_answers, answer_space = answer_space)\n",
    "train_df = train_df[train_df[\"proceed\"] == 1].reset_index(drop = True).drop([\"proceed\"], axis = 1)\n",
    "\n",
    "validation_df[\"proceed\"] = validation_df[\"answers\"].apply(check_answers, answer_space = answer_space)\n",
    "validation_df = validation_df[validation_df[\"proceed\"] == 1].reset_index(drop = True).drop([\"proceed\"], axis = 1)\n",
    "\n",
    "test_df[\"proceed\"] = test_df[\"answers\"].apply(check_answers, answer_space = answer_space)\n",
    "test_df = test_df[test_df[\"proceed\"] == 1].reset_index(drop = True).drop([\"proceed\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Processing Images & Questions using CLIP model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of lazy processing of images and questions embeddings and recomputing them over and over during forward passes in the model, we can preprocess them and save them in a file using Pickle. This will save us a lot of time when we want to train our model and decrease the time taken by one epoch drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocessor = clip.load(MODEL_NAME, device = DEVICE)\n",
    "clip_model.eval().requires_grad_(False)\n",
    "\n",
    "training_images = process_images(train_df, TRAIN_PATH, clip_model, preprocessor, DEVICE)\n",
    "training_questions = process_questions(train_df, clip_model, DEVICE)\n",
    "with open(OUTPUT_PATH + 'training_images.pkl', 'wb') as f:\n",
    "    pickle.dump(training_images, f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(training_questions, f)\n",
    "\n",
    "validation_images = process_images(validation_df, VALIDATION_PATH, clip_model, preprocessor, DEVICE)\n",
    "validation_questions = process_questions(validation_df, clip_model, DEVICE)\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_images, f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_questions, f)\n",
    "\n",
    "test_images = process_images(test_df, TRAIN_PATH, clip_model, preprocessor, DEVICE)\n",
    "test_questions = process_questions(test_df, clip_model, DEVICE)\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'wb') as f:\n",
    "    pickle.dump(test_images, f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'wb') as f:\n",
    "    pickle.dump(test_questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Creating Dataset Class](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch requires using Dataset class. We will create a class that will be used to load the data and process it during training. We will also use this class to load the preprocessed images and questions embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, dataframe, encoder_label, images_features = torch.tensor([]), questions_features = torch.tensor([])):\n",
    "        super(VizWizDataset, self).__init__()\n",
    "\n",
    "        # Saving image & question embeddings\n",
    "        self.images_features = images_features\n",
    "        self.questions_features = questions_features\n",
    "        self.answerable = dataframe['answerable'].to_numpy()\n",
    "        self.encoder_answer = encoder_label\n",
    "        self.encoder_answer_type = {\"number\": 0, \"other\": 1, \"unanswerable\": 2, \"yes/no\": 3}\n",
    "\n",
    "        # Saving the dataframe\n",
    "        self.dataframe = dataframe\n",
    "        self.dataframe[\"label\"] = self.dataframe[\"answers\"].apply(self.transform_labels)\n",
    "        self.dataframe[\"one_hot_encoding_answer\"] = self.dataframe[\"label\"].apply(self.one_hot_encoding_answer)\n",
    "        self.dataframe[\"one_hot_encoding_answer_type\"] = self.dataframe[\"answer_type\"].apply(self.one_hot_encoding_answer_type)\n",
    "        self.dataframe[\"label\"] = self.dataframe[\"label\"].apply(self.correcting_labels)\n",
    "\n",
    "    def transform_labels(self, answers):\n",
    "\n",
    "        answers = pd.DataFrame(answers)\n",
    "        answers = answers.answer.tolist()\n",
    "        answers = [self.encoder_answer[w] for w in answers if w in self.encoder_answer.keys()]\n",
    "        return answers\n",
    "\n",
    "    def one_hot_encoding_answer(self, label):\n",
    "        size = len(self.encoder_answer)\n",
    "        encoding_answer = np.zeros(size)\n",
    "        encoding_answer[label] = 1\n",
    "        return encoding_answer\n",
    "\n",
    "    def one_hot_encoding_answer_type(self, answer_type):\n",
    "        size = len(self.encoder_answer_type)\n",
    "        encoding_answer = np.zeros(size)\n",
    "        encoding_answer[self.encoder_answer_type[answer_type]] = 1\n",
    "        return encoding_answer\n",
    "\n",
    "    def correcting_labels(self, label):\n",
    "\n",
    "        len_label = len(label)\n",
    "\n",
    "        if len_label == 10:\n",
    "            return label\n",
    "\n",
    "        else:\n",
    "            preenche = [-1]*(10-len_label)\n",
    "            label = label + preenche\n",
    "            return label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        answer = torch.tensor(self.dataframe[\"one_hot_encoding_answer\"][index], dtype=torch.float32)\n",
    "        answer_type = torch.tensor(self.dataframe[\"one_hot_encoding_answer_type\"][index], dtype=torch.float32)\n",
    "        answer_counter = torch.tensor(self.dataframe[\"label\"][index], dtype=torch.long)\n",
    "        answerable = torch.tensor(self.answerable[index], dtype=torch.float32)\n",
    "        return self.images_features[index], self.questions_features[index], answer, answer_type, answer_counter, answerable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Building Model's Architecture](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our model's architecture according to the paper. We will use PyTorch to build our model as we said before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:08:52.934991Z",
     "iopub.status.busy": "2023-06-22T22:08:52.934623Z",
     "iopub.status.idle": "2023-06-22T22:08:53.005992Z",
     "shell.execute_reply": "2023-06-22T22:08:53.004837Z",
     "shell.execute_reply.started": "2023-06-22T22:08:52.934962Z"
    }
   },
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, hidden_size, model_name = \"ViT-L/14@336px\", device = torch.device(\"cpu\")):\n",
    "        super(VQAModel, self).__init__()\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "\n",
    "        self.vizwiz_training_accuracies = []\n",
    "        self.vizwiz_validation_accuracies = []\n",
    "\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Loading the CLIP model\n",
    "        self.clip_model, self.preprocess = clip.load(model_name, device = device)\n",
    "\n",
    "        # Freezing the CLIP model\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # First linear layer\n",
    "        self.linear_layer1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1]),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.clip_model.visual.output_dim + self.clip_model.text_projection.shape[1], hidden_size)\n",
    "        )\n",
    "\n",
    "        # Second linear layer\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "        self.answer_type_layer = nn.Linear(hidden_size, 4)\n",
    "        self.answer_mask_layer = nn.Linear(4, num_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image, question):\n",
    "\n",
    "        # Flattening and concatenating the image and question features\n",
    "        image = torch.flatten(image, start_dim=1)\n",
    "        question = torch.flatten(question, start_dim=1)\n",
    "        features = torch.cat((image, question), dim=1)\n",
    "\n",
    "        # Passing the features through the first linear layer\n",
    "        features = self.linear_layer1(features)\n",
    "\n",
    "        # Passing the features to get 4 answer types\n",
    "        answer_type = self.answer_type_layer(features)\n",
    "\n",
    "        # Expanding answer make to the same size as the number of classes (vocab size)\n",
    "        answer_mask = self.answer_mask_layer(answer_type)\n",
    "\n",
    "        # Applying sigmoid to get the answer mask\n",
    "        answer_mask = self.sigmoid(answer_mask)\n",
    "\n",
    "        # Passing the features through the second linear layer\n",
    "        output = self.linear_layer2(features)\n",
    "\n",
    "        # Applying the answer mask to the output\n",
    "        output = output * answer_mask\n",
    "\n",
    "        #return output, answer_type, answerability_score\n",
    "        return output, answer_type\n",
    "\n",
    "    def train_model(self, training_dataloader, validation_dataloader, test_dataloader, criterion, optimizer, epochs = 10, save_path = None, save_every = 1):\n",
    "        for epoch in range(1,epochs+1):\n",
    "            #training_loss, training_accuracy, training_vizwiz_accuracy, train_answerability_score = self.training_step(training_dataloader, criterion, optimizer, self.device)\n",
    "            training_loss, training_vizwiz_accuracy = self.training_step(training_dataloader, criterion, optimizer, self.device)\n",
    "\n",
    "            #validation_loss, validation_accuracy, validation_vizwiz_accuracy, validation_answerability_score = self.validation_step(validation_dataloader, criterion, self.device)\n",
    "            validation_loss, validation_vizwiz_accuracy = self.validation_step(validation_dataloader, criterion, self.device)\n",
    "\n",
    "            #test_accuracy, test_vizwiz_accuracy, test_answerability_score = self.test_step(test_dataloader)\n",
    "            test_vizwiz_accuracy = self.test_step(test_dataloader)\n",
    "\n",
    "            self.training_losses.append(training_loss)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "\n",
    "            self.vizwiz_training_accuracies.append(training_vizwiz_accuracy)\n",
    "            self.vizwiz_validation_accuracies.append(validation_vizwiz_accuracy)\n",
    "\n",
    "            print(\"Epoch: {} | Training Loss: {:.3f} | Validation Loss: {:.3f}\".format(epoch, training_loss, validation_loss))\n",
    "            #print(\"Epoch: {} | Training Accuracy: {:.3f} | Validation Accuracy: {:.3f} | Test Accuracy: {:.3f}\".format(epoch, training_accuracy, validation_accuracy, test_accuracy))\n",
    "            print(\"Epoch: {} | Training VizWiz Accuracy: {:.3f} | Validation VizWiz Accuracy: {:.3f} | Test VizWiz Accuracy: {:.3f}\".format(epoch, training_vizwiz_accuracy, validation_vizwiz_accuracy, test_vizwiz_accuracy))\n",
    "            #print(\"Epoch: {} | Training Answerability Score: {:.3f} | Validation Answerability Score: {:.3f} | Test Answerability Score: {:.3f}\\n\".format(epoch, train_answerability_score, validation_answerability_score, test_answerability_score))\n",
    "\n",
    "            logs = open(save_path+'logs.txt', 'a')\n",
    "            log_epoch = f'Epoch {epoch} \\t Training Loss: {training_loss} \\t Training Acc: {training_vizwiz_accuracy} \\t Validation Loss: {validation_loss} \\t Validation Acc: {validation_vizwiz_accuracy} \\t Test Acc: {test_vizwiz_accuracy}'+\"\\n\"\n",
    "            logs.write(log_epoch)\n",
    "            logs.close()\n",
    "\n",
    "            if save_path != None and epoch % save_every == 0:\n",
    "                self.save_model(save_path + \"epoch_{}.pth\".format(epoch))\n",
    "        return\n",
    "\n",
    "    def training_step(self, dataloader, criterion, optimizer, device):\n",
    "        #training_loss, training_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        training_loss, vizwiz_accuracy, total_sum = 0.0, 0.0, 0\n",
    "\n",
    "        self.train()\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "            #image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "            image, question, answer, answer_type, answers_for_questions = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "            output, answer_type_predicted = self.forward(image, question)\n",
    "            #answerable = 1 - answerable\n",
    "            #answerable_predict = 1.0 - answerable_predict\n",
    "            #loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "            loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "            predicted_answer = torch.argmax(output, dim = 1)\n",
    "\n",
    "            ### ATENÇÃO: PEGA QUALQUER POSIÇÃO QUE CONTENHA UM\n",
    "            actual_answer = torch.argmax(answer, dim = 1)\n",
    "\n",
    "            for i in range(len(answer)):\n",
    "               \n",
    "                total_sum +=1\n",
    "                vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                \n",
    "        training_loss /= len(dataloader)\n",
    "        #training_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        #return training_loss, training_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "        return training_loss, vizwiz_accuracy\n",
    "\n",
    "\n",
    "    def validation_step(self, dataloader, criterion, device):\n",
    "        #validation_loss, validation_accuracy, vizwiz_accuracy, total_sum = 0.0, 0.0, 0.0, 0\n",
    "        validation_loss, vizwiz_accuracy, total_sum = 0.0, 0.0, 0\n",
    "\n",
    "        #answerable_true = []\n",
    "        #answerable_predicted = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                #image, question, answer, answer_type, answers_for_questions, answerable = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device), answerable.to(device)\n",
    "                image, question, answer, answer_type, answers_for_questions = image.to(device), question.to(device), answer.to(device), answer_type.to(device), answers_for_questions.to(device)\n",
    "\n",
    "                #output, answer_type_predicted, answerable_predict = self.forward(image, question)\n",
    "                output, answer_type_predicted = self.forward(image, question)\n",
    "\n",
    "                # Answerablity is the confidence that quesion is not answerable, so we have to subtract from 1\n",
    "                #answerable = 1 - answerable\n",
    "                #answerable_predict = 1.0 - answerable_predict\n",
    "                #loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type) + self.answerability_loss_fn(answerable_predict, answerable)\n",
    "                loss = criterion(output, answer) + criterion(answer_type_predicted, answer_type)\n",
    "\n",
    "                validation_loss += loss.item()\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                   \n",
    "                    total_sum +=1\n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    #answerable_true.append(answerable[i].item())\n",
    "                    #answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        #answerable_true = np.array(answerable_true)\n",
    "        #answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        validation_loss /= len(dataloader)\n",
    "        #validation_accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "\n",
    "        # We will use weighted average since that there is imbalance in answerability in the dataset as displayed in EDA section\n",
    "        #return validation_loss, validation_accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "        return validation_loss, vizwiz_accuracy\n",
    "\n",
    "    def test_step(self, dataloader):\n",
    "        self.eval()\n",
    "        #accuracy, total_sum, vizwiz_accuracy = 0.0, 0, 0.0\n",
    "        total_sum, vizwiz_accuracy = 0, 0.0\n",
    "        #answerable_true = []\n",
    "        #answerable_predicted = []\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(dataloader):\n",
    "                image, question, answer, answer_type, answers_for_questions, answerable = batch\n",
    "                #image, question, answer, answer_type, answers_for_questions, answerable = image.to(self.device), question.to(self.device), answer.to(self.device), answer_type.to(self.device), answers_for_questions.to(self.device), answerable.to(self.device)\n",
    "                image, question, answer, answer_type, answers_for_questions = image.to(self.device), question.to(self.device), answer.to(self.device), answer_type.to(self.device), answers_for_questions.to(self.device)\n",
    "\n",
    "                #output, _, answerable_predict = self.forward(image, question)\n",
    "                output, _ = self.forward(image, question)\n",
    "                #answerable = 1 - answerable\n",
    "                #answerable_predict = 1.0 - answerable_predict\n",
    "                predicted_answer = torch.argmax(output, dim = 1)\n",
    "                actual_answer = torch.argmax(answer, dim = 1)\n",
    "                for i in range(len(answer)):\n",
    "                   \n",
    "                    vizwiz_accuracy += min(1, torch.sum(torch.eq(predicted_answer[i], answers_for_questions[i])).item()/3)\n",
    "                    total_sum +=1\n",
    "                    #answerable_true.append(answerable[i].item())\n",
    "                    #answerable_predicted.append(answerable_predict[i].item())\n",
    "\n",
    "        #answerable_true = np.array(answerable_true)\n",
    "        #answerable_predicted = np.array(answerable_predicted)\n",
    "\n",
    "        #accuracy /= total_sum\n",
    "        vizwiz_accuracy /= total_sum\n",
    "        #return accuracy, vizwiz_accuracy, average_precision_score(answerable_true, answerable_predicted, average = 'weighted')\n",
    "        return vizwiz_accuracy\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saves the model state dictionary to the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to save the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model state dictionary from the given path.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - path (str): the path to load the model state dictionary\n",
    "\n",
    "        Returns:\n",
    "        - self: the loaded model object\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        return self\n",
    "\n",
    "    def predict(self, image, question):\n",
    "        \"\"\"\n",
    "        Predicts the output and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image (tensor): the image tensor\n",
    "        - question (tensor): the question tensor\n",
    "\n",
    "        Returns:\n",
    "        - output (tensor): the predicted output tensor\n",
    "        - answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        #output, answer_type, answerability = self.forward(image, question)\n",
    "        output, answer_type = self.forward(image, question)\n",
    "        #answerability = 1.0 - answerability\n",
    "        #return output, answer_type, answerability\n",
    "        return output, answer_type\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation losses.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_losses, label = \"Training Loss\")\n",
    "        plt.plot(self.validation_losses, label = \"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_vizwiz_accuracy(self):\n",
    "        \"\"\"\n",
    "        Plots the VizWiz training and validation accuracies.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.vizwiz_training_accuracies, label = \"VizWiz Training Accuracy\")\n",
    "        plt.plot(self.vizwiz_validation_accuracies, label = \"VizWiz Validation Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_answerability(self):\n",
    "        \"\"\"\n",
    "        Plots the training and validation answerabilities.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        plt.plot(self.training_answerability, label = \"Training Answerability\")\n",
    "        plt.plot(self.validation_answerability, label = \"Validation Answerability\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def test_model(self, image_path, question):\n",
    "        \"\"\"\n",
    "        Tests the model by predicting the answer and answer type for the given image and question.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "        - image_path (str): the path to the image file or URL\n",
    "        - question (str): the question to be asked\n",
    "\n",
    "        Returns:\n",
    "        - predicted_answer (tensor): the predicted answer tensor\n",
    "        - predicted_answer_type (str): the predicted answer type\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        if image_path.startswith(\"http\"):\n",
    "            image = Image.open(requests.get(image_path, stream = True).raw)\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        image_features = self.clip_model.encode_image(image)\n",
    "        image_features = torch.flatten(image_features, start_dim=1)\n",
    "\n",
    "        question =  clip.tokenize(question).to(self.device)\n",
    "        text_features = self.clip_model.encode_text(question).float()\n",
    "        text_features = torch.flatten(text_features, start_dim=1)\n",
    "\n",
    "        #predicted_answer, predicted_answer_type, answerability = self.predict(image_features, text_features)\n",
    "        predicted_answer, predicted_answer_type = self.predict(image_features, text_features)\n",
    "\n",
    "        #return predicted_answer, predicted_answer_type, answerability\n",
    "        return predicted_answer, predicted_answer_type\n",
    "\n",
    "    def print_CLIP_model(self):\n",
    "        \"\"\"\n",
    "        Prints the details of the selected CLIP model.\n",
    "\n",
    "        Args:\n",
    "        - self: the model object\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        input_resolution = self.clip_model.visual.input_resolution\n",
    "        context_length = self.clip_model.context_length\n",
    "        vocab_size = self.clip_model.vocab_size\n",
    "\n",
    "        print(\"Selected model:\", self.model_name)\n",
    "        print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in self.clip_model.parameters()]):,}\")\n",
    "        print(\"Input resolution:\", input_resolution)\n",
    "        print(\"Context length:\", context_length)\n",
    "        print(\"Vocab size:\", vocab_size)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Loading Preprocessed Embeddings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:08:55.516451Z",
     "iopub.status.busy": "2023-06-22T22:08:55.516038Z",
     "iopub.status.idle": "2023-06-22T22:09:12.361279Z",
     "shell.execute_reply": "2023-06-22T22:09:12.360245Z",
     "shell.execute_reply.started": "2023-06-22T22:08:55.516418Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_PATH + 'training_images.pkl', 'rb') as f:\n",
    "    training_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'training_questions.pkl', 'rb') as f:\n",
    "    training_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'validation_images.pkl', 'rb') as f:\n",
    "    validation_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'validation_questions.pkl', 'rb') as f:\n",
    "    validation_questions = pickle.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH + 'test_images.pkl', 'rb') as f:\n",
    "    test_images = pickle.load(f)\n",
    "with open(OUTPUT_PATH + 'test_questions.pkl', 'rb') as f:\n",
    "    test_questions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[Preparing Data Loaders](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:09:12.363829Z",
     "iopub.status.busy": "2023-06-22T22:09:12.363447Z",
     "iopub.status.idle": "2023-06-22T22:10:01.685916Z",
     "shell.execute_reply": "2023-06-22T22:10:01.684889Z",
     "shell.execute_reply.started": "2023-06-22T22:09:12.363794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constructing the training dataset\n",
    "training_dataset = VizWizDataset(train_df, encoder_label, training_images, training_questions)\n",
    "\n",
    "# Constructing the validation dataset\n",
    "validation_dataset = VizWizDataset(validation_df, encoder_label, validation_images, validation_questions)\n",
    "\n",
    "# Constructing the test dataset\n",
    "test_dataset = VizWizDataset(test_df, encoder_label, test_images, test_questions)\n",
    "\n",
    "# Configuring the data loaders\n",
    "BATCH_SIZE = 32 # 64 is good too but 32 is better (variance wise)\n",
    "\n",
    "# Constructing the training, validation and test data loaders\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[Training the Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:11:37.201389Z",
     "iopub.status.busy": "2023-06-22T22:11:37.200975Z",
     "iopub.status.idle": "2023-06-22T22:18:26.935357Z",
     "shell.execute_reply": "2023-06-22T22:18:26.934296Z",
     "shell.execute_reply.started": "2023-06-22T22:11:37.201357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuring training's hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_CLASSES = len(encoder_label)\n",
    "SAVE_PATH = OUTPUT_PATH\n",
    "SAVE_EVERY = 5\n",
    "\n",
    "# Initializing the model\n",
    "model = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=512, model_name=MODEL_NAME).to(DEVICE)\n",
    "model.print_CLIP_model()\n",
    "\n",
    "# Initializing the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "# Training the model and plotting the loss and accuracy\n",
    "model.train_model(training_dataloader, validation_dataloader, test_dataloader, loss_function, optimizer, epochs=NUM_EPOCHS, save_path=SAVE_PATH, save_every=SAVE_EVERY)\n",
    "model.plot_loss()\n",
    "#model.plot_accuracy()\n",
    "model.plot_vizwiz_accuracy()\n",
    "#model.plot_answerability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model at **epoch number 45** outperforms the same model at any other epochs, so let's pick this model as our ultimate and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_13_'></a>[Remarks](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, the model is very light weight and fast, it takes ~ 1 minute to run an epoch. In addition to this, the model converges very fast, it only takes a maximum of 30 epoch to fully converges. This is due to the fact that we are using CLIP model which is pretrained on a huge dataset.\n",
    "- We can further improve the model by training more models with same architecture but different backbone for CLIP model. We can also use different pretrained models for image and text embeddings and ensemble them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_14_'></a>[Test your own image !](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part of code allows the user to test his own image using the trained model. You just have to configure `IMAGE_PATH` and `QUESTION` variables and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-22T22:30:10.813107Z",
     "iopub.status.busy": "2023-06-22T22:30:10.812592Z",
     "iopub.status.idle": "2023-06-22T22:30:23.077168Z",
     "shell.execute_reply": "2023-06-22T22:30:23.075981Z",
     "shell.execute_reply.started": "2023-06-22T22:30:10.813066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taking a sample image and question from the user\n",
    "QUESTION = \"What kind of food is this?\"\n",
    "IMAGE_PATH = \"train/VizWiz_train_00000008.jpg\"\n",
    "\n",
    "# Loading the model from the disk\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"ViT-L/14@336px\"\n",
    "NUM_CLASSES = len(encoder_label)\n",
    "MODEL_PATH = OUTPUT_PATH+\"/epoch_50.pth\" # OUTPUT_PATH + 'model.pt'\n",
    "model = VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=512, model_name=MODEL_NAME).to(DEVICE)\n",
    "model.load_model(MODEL_PATH)\n",
    "\n",
    "# Predicting the answer and answer type\n",
    "#predicted_answer, predicted_answer_type, answerability = model.test_model(image_path = IMAGE_PATH, question = QUESTION)\n",
    "predicted_answer, predicted_answer_type = model.test_model(image_path = IMAGE_PATH, question = QUESTION)\n",
    "\n",
    "answer = decoder_label[predicted_answer.cpu().detach().numpy()[0].argmax()]\n",
    "#answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "\n",
    "# Printing the answer and answer type\n",
    "#print(\"The Answer is: \" + answer[0][0])\n",
    "print(\"The Answer is: \" + answer)\n",
    "#print(\"The Answer Type is: \" + answer_type[0][0])\n",
    "#print(\"The confidence for being unanswerable: \" + str(answerability.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_15_'></a>[Building Test Answers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"test.json\")\n",
    "df = df[['image', 'question']]\n",
    "\n",
    "# let's create two json objects to store the output of the model then write them to a file\n",
    "model_answers = []\n",
    "#model_answerability = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    image_url = df['image'][i]\n",
    "    question = df['question'][i]\n",
    "    image_path = \"test/\" + image_url\n",
    "    #predicted_answer, predicted_answer_type, answerability = model.test_model(image_path = image_path, question = question)\n",
    "    predicted_answer, predicted_answer_type = model.test_model(image_path = image_path, question = question)\n",
    "\n",
    "    #answer = ANSWER_ONEHOTENCODER.inverse_transform(predicted_answer.cpu().detach().numpy())\n",
    "    answer = decoder_label[predicted_answer.cpu().detach().numpy()[0].argmax()]\n",
    "    #answer_type = ANSWER_TYPE_ONEHOTENCODER.inverse_transform(predicted_answer_type.cpu().detach().numpy())\n",
    "    #answer_result = {'image': image_url, 'answer': answer[0][0]}\n",
    "    answer_result = {'image': image_url, 'answer': answer}\n",
    "    #answerability_result = {'image': image_url, 'answerability': answerability.item()}\n",
    "    model_answers.append(answer_result)\n",
    "    #model_answerability.append(answerability_result)\n",
    "\n",
    "# Writing them using pickle\n",
    "with open('answers_results.json', 'w') as file:\n",
    "    json.dump(model_answers, file)\n",
    "#with open('answerability_results.json', 'w') as file:\n",
    "#    json.dump(model_answerability, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
