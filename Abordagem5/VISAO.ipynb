{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lavis.models import load_model_and_preprocess\n",
    "device = 'cuda'\n",
    "model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain_vitL\", is_eval=True, device=device)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base(name_arq):\n",
    "\n",
    "    f = open(name_arq, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Pega apenas as \"respondiveis\"\n",
    "    #data = [d for d in data if d[\"answerable\"] == 1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e371c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "transform_patches = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize(size=(112,112)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "transform_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_blip2(model, img_tensor, vis_processors, device):\n",
    "\n",
    "    #emb_format = np.empty()\n",
    "    i = 0\n",
    "    for img in img_tensor:\n",
    "\n",
    "        image = transform_pil(img)\n",
    "\n",
    "        #preprocess image\n",
    "        image_processed = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n",
    "\n",
    "        sample = {\"image\": image_processed}\n",
    "\n",
    "        image_emb = model.extract_features(sample, mode=\"image\").image_embeds[0,0,:] # size (768)\n",
    "\n",
    "        if i == 0:\n",
    "          emb_format = image_emb.cpu().numpy().reshape(1,768)\n",
    "\n",
    "        else:\n",
    "          emb_format = np.vstack((emb_format, image_emb.cpu().numpy().reshape(1,768)))\n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "        #list_img.append(image_emb.cpu().numpy())\n",
    "\n",
    "    return emb_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visão\n",
    "\"\"\"\n",
    "{\"imagem.jpg\": [vetores],\n",
    "\"imagem2.jpg\": [vetores],\n",
    "...\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_embedding(embedding_img):\n",
    "\n",
    "  for i in range(len(embedding_img)):\n",
    "\n",
    "    if i==0:\n",
    "      emb_format = np.concatenate((embedding_img[i].reshape(1,768), embedding_img[i+1].reshape(1,768)), axis=0)\n",
    "    else:\n",
    "      emb_format = np.concatenate((emb_format, embedding_img[i].reshape(1,768)), axis=0)\n",
    "\n",
    "  return emb_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d57589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_visao(name_arq, model, tam_base, vis_processors, device):\n",
    "\n",
    "    # Realiza a leitura da base\n",
    "    data = load_base(name_arq)\n",
    "\n",
    "    #data = data[:2]\n",
    "\n",
    "    # Define diretório onde se encontram as imagens\n",
    "    dir_img = name_arq.split(\".json\")[0]\n",
    "\n",
    "    # Irá carregar as informações visuais referentes a cada uma das imagens\n",
    "    info_visao = {}\n",
    "\n",
    "    for info in tqdm(data):\n",
    "\n",
    "        base = []\n",
    "\n",
    "        # Pega o nome da imagem\n",
    "        name_img = info[\"image\"]\n",
    "\n",
    "        # Faz a leitura da imagem\n",
    "        img = Image.open(dir_img+\"/\"+name_img)\n",
    "\n",
    "        # Padroniza a imagem\n",
    "        img_patches = transform_patches(img)\n",
    "        img_tensor = transform_img(img)\n",
    "\n",
    "        # Coloca a imagem e seus patches em uma lista para obter os seus embeddings\n",
    "        base.append(img_tensor)\n",
    "        base.append(img_patches[:, :112, :112])\n",
    "        base.append(img_patches[:, :112, 112:])\n",
    "        base.append(img_patches[:, 112:, :112])\n",
    "        base.append(img_patches[:, 112:, 112:])\n",
    "\n",
    "        # Pega os embeddings referentes a imagem\n",
    "        embedding_img = get_embedding_blip2(model, base, vis_processors, device)\n",
    "\n",
    "        # Atualiza as informações da imagem\n",
    "        info_visao[name_img] = embedding_img\n",
    "\n",
    "        del(embedding_img)\n",
    "\n",
    "    return info_visao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_visao(name_arq_in, info_visao):\n",
    "\n",
    "    name_arq_out = name_arq_in.split(\".json\")[0]+\"_info_visao.pkl\"\n",
    "\n",
    "    file = open(name_arq_out, 'ab')\n",
    "    pickle.dump(info_visao, file, pickle.HIGHEST_PROTOCOL)\n",
    "    file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name_arq = \"test.json\"\n",
    "info_visao = get_info_visao(name_arq, model, 1, vis_processors, device)\n",
    "save_info_visao(name_arq, info_visao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info_visao['VizWiz_train_00000000.jpg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b5c03",
   "metadata": {},
   "source": [
    "#### Fontes\n",
    "\n",
    "https://colab.research.google.com/drive/1jIflL9-gktbXq_2cEE_KM7yYq2PaOKRM?authuser=1#scrollTo=8cRyNhMQaAyh\n",
    "\n",
    "https://khvmaths.medium.com/vision-transformer-understanding-the-underlying-concept-83d699d71180"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
