{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59100b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from spacy import displacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Inicializa modelos\n",
    "\n",
    "#import spacy_transformers\n",
    "#import en_core_web_trf\n",
    "#nlp = en_core_web_trf.load()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "from lavis.models import load_model_and_preprocess\n",
    "device = 'cuda'\n",
    "model, _, txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain_vitL\", is_eval=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d64600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base(name_arq):\n",
    "\n",
    "    f = open(name_arq, encoding=\"utf8\")\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Pega apenas as \"respondiveis\"\n",
    "    #data = [d for d in data if d[\"answerable\"] == 1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(text):\n",
    "\n",
    "    # Expand contractions like \"I'll\" to \"I will\"\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Padroniza todas as palavras para minúsculo\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove \"__\"\n",
    "    text = text.replace(\"_\", \"\")\n",
    "\n",
    "    # Retira espaços extras\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9474d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connections(text):\n",
    "\n",
    "    # Ligações estabelecidas via análise sintática\n",
    "    doc = nlp(text)\n",
    "\n",
    "    text_doc = \"\"\n",
    "    for token in doc:\n",
    "        text_doc = text_doc+\" \"+token.text\n",
    "\n",
    "    len_text = len(text_doc.split())\n",
    "    deps_parse = displacy.parse_deps(doc)\n",
    "\n",
    "    # Já considera a bidirecionalidade\n",
    "    ligacoes_sintatic = [[con[\"start\"], con[\"end\"]] for con in deps_parse[\"arcs\"]]+[[con[\"end\"], con[\"start\"]] for con in deps_parse[\"arcs\"]]\n",
    "\n",
    "    ligacoes = [[]]*len_text\n",
    "\n",
    "    for w in ligacoes_sintatic:\n",
    "\n",
    "        if len(ligacoes[w[0]]) == 0:\n",
    "            ligacoes[w[0]] = [w[1]]\n",
    "        else:\n",
    "            ligacoes[w[0]].append(w[1])\n",
    "\n",
    "    # Ligações de acordo com a bidirecionalidade das palavras (palavras anterior e posterior)\n",
    "    for i in range(len_text):\n",
    "\n",
    "        if i == 0:\n",
    "            ligacoes[i].append(1)\n",
    "\n",
    "        elif i == (len_text-1):\n",
    "            ligacoes[i].append(len_text-2)\n",
    "\n",
    "        else:\n",
    "            ligacoes[i].append(i-1)\n",
    "            ligacoes[i].append(i+1)\n",
    "\n",
    "    ligacoes = [list(set(i)) for i in ligacoes]\n",
    "\n",
    "    return len_text, text_doc.split(), ligacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_blip2(text, model, txt_processors):\n",
    "\n",
    "    list_text = [text]\n",
    "    list_text = list_text + text.split()\n",
    "    emb = []\n",
    "\n",
    "    for w in list_text:\n",
    "\n",
    "        text_input = txt_processors[\"eval\"](w)\n",
    "\n",
    "        # build sample\n",
    "        sample = {\"text_input\": [text_input]}\n",
    "        text_emb = model.extract_features(sample, mode=\"text\").text_embeds[0,0,:] # size (768)\n",
    "        emb.append(text_emb.cpu().numpy())\n",
    "\n",
    "    return np.array(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "\"\"\"\n",
    "{\"imagem.jpg\":{\"len_perg\": x, \"word_0\": {\"word\": word, \"ligacoes\": [1, 3]}, ...., \"word_n\": {\"word\": word, \"ligacoes\": [n-1, n-4]}, \"embedding\": [vetor]},\n",
    "...\n",
    "\"imagem.jpg\":{\"len_perg\": x, \"word_0\": {\"word\": word, \"ligacoes\": [1, 3]}, ...., \"word_n\": {\"word\": word, \"ligacoes\": [n-1, n-4]}, \"embedding\": [vetor]},\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58560531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_nlp(name_arq, model, txt_processors, tam_base):\n",
    "\n",
    "    # Realiza a leitura da base\n",
    "    data = load_base(name_arq)\n",
    "\n",
    "    #data = data[:2]\n",
    "\n",
    "    # Irá carregar as informações textuais referentes a cada uma das imagens\n",
    "    info_nlp = {}\n",
    "\n",
    "    for info in tqdm(data):\n",
    "\n",
    "        # Inicializa o dicionário referente a dada imagem\n",
    "        info_nlp[info[\"image\"]] = {}\n",
    "\n",
    "        # Pega a pergunta e realiza pré-processamento em cima dela\n",
    "        perg = info[\"question\"]\n",
    "        perg = Preprocessing(perg)\n",
    "\n",
    "        # Estabelece as conexões de cada palavra dentro da pergunta\n",
    "        len_text, words_list, ligacoes = get_connections(perg)\n",
    "\n",
    "        perg = ' '.join(words_list)\n",
    "\n",
    "        # Calcula o tamanho da pergunta, ou seja, quantidade de palavras\n",
    "        info_nlp[info[\"image\"]][\"len_perg\"] = len_text\n",
    "\n",
    "        # Adiciona as informações calculadas até o momento no dicionário referente a imagem em análise\n",
    "        for i in range(len_text):\n",
    "\n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)] = {}\n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)][\"word\"] = words_list[i]\n",
    "            info_nlp[info[\"image\"]][\"word_\"+str(i)][\"ligacoes\"] = ligacoes[i]\n",
    "\n",
    "        # Calcula os embeddings das palavras e adiciona ao dicionário\n",
    "        embeddings = get_embedding_blip2(perg, model, txt_processors)\n",
    "\n",
    "        info_nlp[info[\"image\"]][\"embeddings\"] = embeddings\n",
    "\n",
    "    return info_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_nlp(name_arq_in, info_nlp):\n",
    "\n",
    "    name_arq_out = name_arq_in.split(\".json\")[0]+\"_info_nlp\"\n",
    "\n",
    "    file = open(name_arq_out, 'wb')\n",
    "    pickle.dump(info_nlp, file)\n",
    "    file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed287df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name_arq = \"val.json\"\n",
    "info_nlp = get_info_nlp(name\n",
    "_arq, model, txt_processors, 5)\n",
    "save_info_nlp(name_arq, info_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cc7ba",
   "metadata": {},
   "source": [
    "#### Fontes\n",
    "\n",
    "https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/\n",
    "\n",
    "https://spacy.io/api/dependencyparser\n",
    "    \n",
    "https://spacy.io/usage/visualizers\n",
    "    \n",
    "https://python.plainenglish.io/how-to-generate-word-embedding-using-bert-2b9e79c27396\n",
    "\n",
    "https://peaceful0907.medium.com/sentence-embedding-by-bert-and-sentence-similarity-759f7beccbf1\n",
    "\n",
    "https://spacy.io/api/top-level#displacy.parse_deps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
